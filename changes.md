# Changes

## 2026-02-23 â€” Conversation sharing

**What:** Added per-conversation public sharing via unique URL. Owners toggle sharing on/off; visitors get a read-only view with no login required.

**Files:**
- `db.py` â€” added `share_token VARCHAR(32) UNIQUE` and `is_public BOOLEAN DEFAULT FALSE` columns to `conversations`
- `api_chat.py` â€” updated `list_conversations` to return share fields; added `POST /conversations/{conv_id}/share` toggle endpoint; added `GET /share/{share_token}` public endpoint
- `frontend/src/api.ts` â€” added `toggleConversationShare` and `fetchSharedConversation` functions
- `frontend/src/pages/ChatLayout.tsx` â€” added `share_token`/`is_public` to `Conversation` interface; added `handleShare` callback; passes `onShare` to Sidebar
- `frontend/src/components/Sidebar.tsx` â€” added share button (ğŸ”—) per conversation row with inline panel (toggle + URL + copy button)
- `frontend/src/App.tsx` â€” added `/share/:shareToken` route
- `frontend/src/pages/SharedConversationPage.tsx` â€” created; public read-only conversation view using existing `MessageBubble` component

## 2026-02-23 â€” Improve chart generation completeness via better prompting

**What:** The `_polish_script` rewriter was silently dropping visual elements (e.g. ç¬” stroke lines, ä¸­æ¢ zones) when simplifying scripts. Added visual-completeness rules and a two-step enumerate-then-rewrite instruction so all requested overlays are always preserved and implemented.

**Files:**
- `tools/ta_executor.py` â€” modified `_SCRIPT_RULES`, `_polish_script` prompt, `_rewrite_script` prompt

**Details:**
- `_SCRIPT_RULES`: added VISUAL COMPLETENESS RULES section with Plotly trace-type reference for common TA elements (fractal markers, stroke lines with None separators, pivot zone shapes, signals) and explicit "never omit an overlay" rule
- `_polish_script`: added Step 1 (enumerate all visual elements) before Step 2 (rewrite), with instruction to fix broken elements rather than delete them
- `_rewrite_script`: added same preservation instruction â€” fix the error, do not remove traces/shapes

## 2026-02-23 â€” Fix messages going to wrong conversation on mobile

**What:** When `POST /api/chat/send` received no `conversation_id`, the backend fell through to `get_active_conversation` which reused the most recent existing conversation. On mobile, tab discards after backgrounding cause the frontend to reload with `activeId = null`, triggering this path. Now a new conversation is created explicitly instead.

**Files:**
- `api_chat.py` â€” modified `send_message()`: create new conversation when `conversation_id` is null; simplify auto-title to query by `run.conv_id` directly

**Details:**
- Added `else: target_conv_id = await new_conversation(user_id)` branch so `run.conv_id` is always set at `AgentRun` creation
- Auto-title code now queries `WHERE id = $1` using `run.conv_id` instead of `ORDER BY updated_at DESC LIMIT 1` â€” removes race condition and the now-dead `run.conv_id is None` branch

## 2026-02-23 â€” Add A-share backtesting rules

**What:** Added A-share specific backtesting rules document, injected it into the planning prompt, and captured `run_ta_script` stdout for returning backtest statistics.

**Files:**
- `data/backtest_rules.md` â€” created: board detection by code prefix (Â±10%/Â±20%/Â±30%), T+1 rule, long-only, next-bar-open execution, æ¶¨åœ/è·Œåœ chart markers, required stats output format
- `config.py` â€” added `_load_backtest_rules()` helper; converted `get_planning_prompt()` to f-string; appended `## å›æµ‹è§„åˆ™ï¼ˆAè‚¡ï¼‰` section from file
- `tools/ta_executor.py` â€” `run_ta_script` now captures `result.stdout` and returns it as `"text"` field alongside the chart file

## 2026-02-23 â€” Add run_ta_script tool with subprocess sandbox and 3-attempt retry

**What:** Created `tools/ta_executor.py` providing a sandboxed Python execution environment for LLM-generated technical analysis scripts, plus full unit test coverage.

**Files:**
- `tools/ta_executor.py` â€” created; `run_ta_script` async function + `RUN_TA_SCRIPT_SCHEMA`, `_make_wrapper_script`, `_rewrite_script` helpers
- `tests/test_ta_executor.py` â€” created; 5 pytest-asyncio tests covering sandbox allowlist, sandbox blocklist, success-on-first-attempt, 3-attempt exhaustion, and success-on-second-attempt
- `requirements.txt` â€” added `pandas-ta` and `plotly`

**Details:**
- Sandbox uses `sys._getframe(1)` to determine whether an import originates from user code (`<string>`) vs library internals (site-packages path); only user-level imports are checked against the allowlist/blocklist
- `subprocess.run` is wrapped with `asyncio.to_thread` to avoid blocking the uvicorn event loop
- On script failure, `_rewrite_script` calls MiniMax to produce a fixed version; retries up to `_MAX_RETRIES=3` times
- Output path follows the same `output/{user_id}/ta_{code}_{YYYYMMDD}_{4char}.html` convention as `output.py`
- `fetch_ohlcv` is awaited directly inside `run_ta_script` (no `run_until_complete`)

## 2026-02-23 â€” Add ta_strategies lookup/save/update tools with tests

**What:** Created the TA strategy knowledge base tools (`lookup_ta_strategy`, `save_ta_strategy`, `update_ta_strategy`) backed by the `ta_strategies` Postgres table, plus full unit test coverage with mocked DB pool.

**Files:**
- `tools/ta_strategies.py` â€” created; implements three async tool functions and their JSON schemas
- `tests/test_ta_strategies.py` â€” created; 5 pytest-asyncio tests covering found/not-found lookup, save, and update (found/not-found)

**Details:**
- Lookup uses FTS on `name` via `to_tsvector('simple', ...)` plus `= ANY(aliases)` fallback and exact case-insensitive name match (GIN index only covers `name` â€” alias search done at query time)
- Save uses `INSERT ... ON CONFLICT (name) DO UPDATE` for upsert behaviour
- Update builds dynamic SET clause from an `allowed` whitelist; detects `UPDATE 0` to return `not_found`
- `parameters` JSONB field serialised via `json.dumps` before passing to asyncpg
- Tests mock `get_pool` at the module level; all 5 pass

## 2026-02-22 â€” Codebase cleanup + server deployment doc

**What:** Removed obsolete files and dead code; rewrote `switchingvps.md` into a comprehensive server deployment guide.

**Files deleted:**
- `.running` â€” temp file containing stale secrets
- `1` â€” empty stray file
- `todo.md` â€” empty
- `test_groq.py` â€” test artifact, not part of app
- `generate_guide_pdf.py` â€” one-off script with hardcoded paths
- `archive/bot.py` â€” old Telegram bot, replaced by web UI

**Code removed:**
- `tools/output.py` â€” `generate_references_image()` (only used by archived bot) + unused `textwrap` import
- `accounts.py` â€” `get_or_create_user()` (only used by archived bot)
- `config.py` â€” `GROK_MODEL_REASONING` (defined but never used anywhere)

**Docs:**
- `switchingvps.md` â€” rewritten into full deployment guide covering fresh setup, systemd, nginx/SSL, cron jobs, firewall, and server migration

---

## 2026-02-22 â€” Add fetch_ohlcv tool for 5-min OHLCV / technical analysis

**What:** Created a new tool that queries the local `ohlcv_5m` table in the marketdata DB, returning candlestick bars with pre-computed MA5/MA20/MA60 and chart-ready series for use with `generate_chart`.

**Files:**
- `tools/ohlcv.py` â€” created; `fetch_ohlcv` + `FETCH_OHLCV_SCHEMA`
- `tools/__init__.py` â€” registered tool
- `config.py` â€” added `fetch_ohlcv` to planning prompt tool table and TA guidance

---

## 2026-02-22 â€” Restrict debate to button-only; clean up field-name translation

**What:** Removed `analyze_trade_opportunity` from the agent's tool list so it can only be triggered by the hypothesis debate button. Also moved the financials column glossary out of config.py and into `data/financials_columns.md`.

**Files:**
- `tools/__init__.py` â€” removed `ANALYZE_TRADE_SCHEMA` from `TOOL_SCHEMAS` (stays in `TOOL_MAP` for debate path)
- `config.py` â€” replaced hardcoded field-name table with `_load_file("financials_columns.md")` helper

---

## 2026-02-22 â€” Fix token-by-token streaming in frontend

**What:** The backend was already emitting `token` SSE events per token, but the frontend SSE reader had no handler for them so they were silently dropped. Wired up proper streaming so text appears incrementally.

**Files:**
- `frontend/src/api.ts` â€” add `onToken?` to `SSECallbacks`; handle `token` SSE events in `_readSSEStream`
- `frontend/src/components/ChatView.tsx` â€” add `streamingContent` state; wire `onToken` in both `handleSend` and `reconnect`; render live streaming bubble; clear on `onDone`/`onError`/`handleStop`

---

## 2026-02-22 â€” Show response elapsed time in chat

**What:** Timed each agent/debate run and displayed elapsed seconds as small muted text at the bottom of every assistant message.

**Files:**
- `api_chat.py` â€” record `t0` before run, compute elapsed, log it, include `elapsed_seconds` in `done` SSE payload
- `frontend/src/api.ts` â€” add `elapsed_seconds?: number` to `onDone` callback type
- `frontend/src/components/ChatView.tsx` â€” thread `elapsedSeconds` through `Message` interface and `MessageBubble` props
- `frontend/src/components/MessageBubble.tsx` â€” render `<div class="message-elapsed">` when present
- `frontend/src/styles/index.css` â€” add `.message-elapsed` style (small, muted, right-aligned)

---

## 2026-02-22 â€” Add Fireworks AI as switchable MiniMax provider

**What:** Added support for running MiniMax via Fireworks AI (drop-in OpenAI-compatible endpoint) with a single env-var toggle to switch between providers.

**Files:**
- `config.py` â€” added `FIREWORKS_API_KEY`, `FIREWORKS_BASE_URL`, `FIREWORKS_MINIMAX_MODEL`, `MINIMAX_PROVIDER` env vars and `get_minimax_config()` helper
- `agent.py` â€” uses `get_minimax_config()` instead of raw constants
- `tools/subagent.py` â€” uses `get_minimax_config()` instead of raw constants
- `tools/trade_analyzer.py` â€” uses `get_minimax_config()` instead of raw constants

**Details:**
- Default provider is `fireworks`; set `MINIMAX_PROVIDER=minimax` to revert to the official API
- Default Fireworks model: `accounts/fireworks/models/minimax-m2p1` (override via `FIREWORKS_MINIMAX_MODEL`)
- Requires `FIREWORKS_API_KEY` in `.env` when using Fireworks provider

## 2026-02-22 â€” Add fetch_baostock_financials tool; fix marketdata DB connection

**What:** Created a new agent-callable tool to query the local BaoStock `financials` table in the marketdata DB. Fixed a bug where `_get_financial_context` was querying the wrong DB (main myaiagent DB instead of marketdata DB), causing silent failures.

**Files:**
- `config.py` â€” added `MARKETDATA_URL` constant; added `fetch_baostock_financials` row to planning prompt tool table; added routing notes for DuPont/cash quality analysis; added citation URL
- `db.py` â€” added `MARKETDATA_URL` import, `marketdata_pool` global, `get_marketdata_pool()` function
- `tools/financials_db.py` â€” created: `fetch_baostock_financials` tool with full column docs, default column set, validation; queries `financials` table via `get_marketdata_pool()`
- `tools/__init__.py` â€” registered `fetch_baostock_financials` in TOOL_SCHEMAS and TOOL_MAP
- `tools/sina_reports.py` â€” fixed `_get_financial_context` to use `get_marketdata_pool()` instead of `get_pool()` (main DB)

**Details:**
- `financials` table lives in the `marketdata` DB (BaoStock source), not the main `myaiagent` DB; the old code was querying the wrong pool and silently returning empty data
- The new tool exposes 30+ columns across 6 categories: profitability, operational efficiency, growth, solvency, cash flow, DuPont decomposition
- Column descriptions from `data/financials_columns.md` are embedded in both the tool schema description and the response `columns_doc` field so the agent always knows what each metric means
- `get_marketdata_pool()` lazily creates a pool on first use (no startup cost if unused)

## 2026-02-22 â€” Remove report file cache (always fetch live)

**What:** Removed the `output/reports/` disk cache and `report_cache` DB lookups from `fetch_company_report`. Reports are now always fetched fresh from Sina Finance on every call.

**Files:**
- `tools/sina_reports.py` â€” removed `_REPORTS_BASE`, `_get_cache_path`, `_check_report_cache`, `_save_report_cache`; removed `from pathlib import Path`; stripped cache-check fast-path and file-write from `fetch_company_report`; removed `cache_path` from return dict

## 2026-02-22 â€” Real-time think-block streaming for all agent LLM calls

**What:** Extended streaming to cover `<think>` content and all non-main-loop LLM calls so users see something immediately even while the model is thinking.

**Files:**
- `agent.py` â€” updated `_stream_llm_response` with a state-machine that streams `<think>` tokens in real-time via a new `on_thinking_chunk` callback; planning turn now uses `_stream_llm_response`; max-turns summary now uses `_stream_llm_response`

**Details:**
- `_stream_llm_response` now has three states: `pre` (buffering to detect `<think>`), `think` (streaming to `on_thinking_chunk` as tokens arrive), `post` (streaming to `on_token`)
- Planning turn: think content streams to source `"agent_plan_think"` / label `"Planning Â· Thinking"` in real-time; the resolved plan still emits as `"Research Plan"` thinking block after
- Main agent loop: each turn pre-computes a think label and source; `<think>` content streams in real-time instead of being buffered until `</think>`; removed the now-redundant post-stream `_emit_thinking` call
- Max-turns summary: was a non-streaming call (final answer blocked until complete); now streams tokens via `_emit_token` and think content via `on_thinking_chunk`

## 2026-02-22 â€” Targeted DB-anchored report analysis (replaces exhaustive summarization)

**What:** Replaced the full-report exhaustive distillation approach with a three-step targeted Q&A flow: pull historical DB financials â†’ generate research questions from trends â†’ answer them from a focused 40k-char chunk of the report.

**Files:**
- `tools/sina_reports.py` â€” removed `_make_chunks`, `_CHUNK_SIZE`, `_CHUNK_OVERLAP`, `_MAX_PARALLEL`; added `_get_financial_context`, `_generate_research_questions`, `_groq_targeted_analysis`; updated `fetch_company_report` slow path to call the three-step flow; updated `FETCH_COMPANY_REPORT_SCHEMA` description

**Details:**
- `_get_financial_context(code)` queries the `financials` DB table for last 8 quarters (ROE, margins, YoY growth, solvency, cash flow, EPS)
- `_generate_research_questions(...)` sends financial trend data to Groq to generate 4â€“6 targeted research questions (e.g. why ROE dropped, what drove the margin compression)
- `_groq_targeted_analysis(...)` prepares a 40k-char focused chunk via `_prepare_report_text`, then answers the questions in a single Groq call with an investment conclusion (çœ‹å¤š/çœ‹ç©º/ä¸­æ€§)
- MD output now includes a "å†å²è´¢åŠ¡èƒŒæ™¯" section with the DB data table, followed by "ç ”ç©¶é—®é¢˜ä¸åˆ†æ" with Groq's answers
- `summarized_by` field is now `"groq_targeted"` on success

## 2026-02-22 â€” Switch report summarizer from Minimax to Groq (PDF-based chunked extraction)

**What:** Replaced Minimax LLM summarization in `sina_reports.py` with Groq `openai/gpt-oss-20b` using full-PDF extraction and parallel chunked processing for higher accuracy and zero hallucination.

**Files:**
- `tools/sina_reports.py` â€” removed Minimax client; added `_download_pdf`, `_extract_pdf_text`, `_make_chunks`, `_groq_summarize_report`; updated `fetch_company_report` to prefer PDF text over HTML bulletin
- `config.py` â€” added `GROQ_API_KEY`, `GROQ_BASE_URL`, `GROQ_REPORT_MODEL`
- `requirements.txt` â€” added `pymupdf`

**Details:**
- PDF downloaded in memory (bytes), text extracted via pymupdf, bytes discarded immediately â€” no temp file on disk
- Report text chunked at 10k chars with 200-char overlap, processed in parallel (semaphore=4)
- Each chunk uses exhaustive extraction prompt (ç©·ä¸¾æå–, no output limit, low temp=0.1)
- Synthesis pass merges all chunks into final structured Markdown
- Falls back to HTML body text if no PDF link found, and to keyword extraction if Groq fails

## 2026-02-21 â€” Market data pipeline (hourly OHLCV for A-shares, plain PostgreSQL)

**What:** Dropped TimescaleDB in favour of plain PostgreSQL 17 (local). Fixed BaoStock API usage (`query_stock_basic()` takes no `fields` arg; `time` field is `YYYYMMDDHHmmssSSS` not `HH:MM:SS`). Pipeline verified end-to-end with 5 stocks.

**Files:**
- `data/setup_db.py` â€” plain PostgreSQL, BRIN index on ts, btree on (code, ts)
- `data/ingest_ohlcv.py` â€” fixed stock list query and timestamp parsing
- `data/update_ohlcv.py` â€” same fixes; added MARKETDATA_URL env var support
- `requirements.txt` â€” added baostock, psycopg2-binary

**Details:**
- Use `MARKETDATA_URL=postgresql://davidc@localhost:5432/marketdata` in .env
- BaoStock fields: `[code, code_name, ipoDate, outDate, type, status]` â€” type at index 4, status at index 5
- Time field slicing: `t[8:10]:t[10:12]:t[12:14]` â†’ HH:MM:SS

---

## 2026-02-21 â€” Market data pipeline (hourly OHLCV for A-shares)

**What:** Added a standalone data pipeline to store 5 years of hourly OHLCV data for 5000+ A-share stocks in a separate PostgreSQL + TimescaleDB `marketdata` database, for use by future backtesting scripts.

**Files:**
- `data/setup_db.py` â€” created: one-time script to create the `marketdata` DB, enable TimescaleDB, create `ohlcv_1h` hypertable with indexes and 7-day compression policy
- `data/ingest_ohlcv.py` â€” created: bulk historical load (2020â€“today), resumable via `.ingest_checkpoint` file, batch inserts with ON CONFLICT DO NOTHING
- `data/update_ohlcv.py` â€” created: daily incremental update script (designed for cron at 16:30 CST), fetches from last ingested timestamp forward
- `requirements.txt` â€” modified: added `baostock` and `psycopg2-binary`

**Details:**
- Data source: BaoStock (`frequency="60"` for 60-min bars), no API key required
- Stock list fetched directly from BaoStock (`query_stock_basic`), no dependency on main DB
- ~25M estimated rows (5yr Ã— 244 days Ã— 4 bars Ã— 5000 stocks), ~500MBâ€“1GB compressed
- BaoStock code format: `sh.600036` â†’ stored as `code="600036"`, `exchange="SH"`
- Timestamps stored as `TIMESTAMPTZ` with `+08:00` offset (CST)
- Cron entry in `update_ohlcv.py` comments: `30 8 * * 1-5` (08:30 UTC = 16:30 CST)

## 2026-02-21 â€” Chitchat routing at planning stage

**What:** Agent now classifies intent at the planning turn and short-circuits for non-financial queries, returning a direct friendly answer without entering the agentic tool loop.

**Files:**
- `config.py` â€” updated `get_planning_prompt()` to prepend INTENT classification instructions
- `agent.py` â€” updated `_run_agent_inner()` to parse `INTENT: chitchat` / `INTENT: finance` from the planning response and return early for chitchat

## 2026-02-21 â€” Speech-to-text (Whisper) + stocknames table

**What:** Added OpenAI Whisper speech-to-text integration (test server) and a `stocknames` table populated daily from SSE, SZSE, and BSE official exchange APIs.

**Files:**
- `tests/test_whisper_web.py` â€” created: standalone FastAPI test server for Whisper STT
- `tests/test_whisper.py` â€” created: CLI connectivity test for Whisper API
- `tests/test_exchange_apis.py` â€” created: inspection script for SSE/SZSE/BSE APIs
- `tools/populate_stocknames.py` â€” created: fetches ~5500 A-share stocks from all 3 exchanges, upserts into DB
- `db.py` â€” modified: added `stocknames` table to SCHEMA_SQL
- `web.py` â€” modified: added `_stocknames_scheduler()` background task (populates on startup if empty, refreshes daily at 19:00)

**Details:**
- SSE fetches ä¸»æ¿Aè‚¡ + ç§‘åˆ›æ¿ via JSON API (requires Referer header)
- SZSE fetches via XLSX download
- BSE fetches from bseinfo.net paginated POST API
- All three fetches run in parallel via `asyncio.gather` + `asyncio.to_thread`
- Upsert is idempotent â€” safe to re-run or re-populate
- `stocknames` columns: stock_code, exchange (SH/SZ/BJ), stock_name, full_name, sector, industry, list_date

## 2026-02-20 â€” Fix MiniMax error 2013 (incomplete tool-call sequences)

**What:** Replaced the front-only trim in `load_recent_messages()` with a full-array scan that drops incomplete or orphaned tool-call sequences anywhere in message history.

**Files:**
- `accounts.py` â€” modified `load_recent_messages()`, added `_repair_tool_call_sequence()`

**Details:**
- Root cause: when user cancels an agent run, the `role:assistant` message with `tool_calls` is saved to DB but the `role:tool` results are not â€” leaving a corrupt sequence in history
- Old code only trimmed orphaned messages at the front of the window; sequences buried in the middle were passed through intact, triggering MiniMax error 2013
- `_repair_tool_call_sequence()` walks the full array: drops any assistant+tool_calls whose expected IDs don't match the immediately-following tool results, and drops any orphaned tool messages

## 2026-02-20 â€” Fix TOC parser for real CN annual report format

**What:** Fixed three bugs that caused `_parse_toc()` to return [] on every real annual report, falling back to the 80k hard-cap with no TOC filtering.

**Files:**
- `tools/sina_reports.py` â€” modified

**Details:**
- Bug 1: `_TOC_ENTRY_RE` used `\s+` (required space) after ç« /èŠ‚, but real reports have no space: `ç¬¬ä¸€ç« å…¬å¸ç®€ä»‹ ...... 9` not `ç¬¬ä¸€ç«  å…¬å¸ç®€ä»‹ ...... 9`. Fixed to `\s*`.
- Bug 2: `_CHAPTER_HEADING_RE` also required `[\s\u3000]` after ç« /èŠ‚, so body-text chapter boundaries were never detected. Fixed by removing the space requirement.
- Bug 3: `_parse_toc` didn't handle plain pre-chapter TOC entries (`é‡è¦æç¤º ...... 1`, `è‘£äº‹ä¼šè‡´è¾ ...... 5` etc.) and didn't anchor on the `ç›®å½•` marker. Rewrote to: (1) find `ç›®å½•` in first 600 lines, (2) parse up to 120 lines after it with both `_TOC_ENTRY_RE` and new `_TOC_PLAIN_ENTRY_RE`, (3) fall back to 400-line scan if no `ç›®å½•` found.
- Added `è‡´è¾`, `è‡´è¯` to `_SKIP_CHAPTER_KEYWORDS` (covers è‘£äº‹ä¼šè‡´è¾, è¡Œé•¿è‡´è¾ etc.)
- Added `_TOC_PLAIN_ENTRY_RE` regex for plain entries; excludes sub-entries starting with ä¸€ã€ï¼ˆä¸€ï¼‰ etc.
- Verified against real ä¸Šæµ·é“¶è¡Œ2024å¹´æŠ¥ TOC: 15/15 chapters detected, all keep/skip flags correct, sub-entries excluded.

## 2026-02-20 â€” Add structure.md architecture reference

**What:** Created a comprehensive architecture document so future coding agents can understand the full system without reading every source file.

**Files:**
- `structure.md` â€” created (full architecture reference)
- `CLAUDE.md` â€” modified (added instruction to read structure.md before writing code)

## 2026-02-20 â€” Stop button now cancels server-side agent task

**What:** The Stop button now sends a `POST /api/chat/stop` request that cancels the background asyncio task, so the agent actually stops running rather than just disconnecting the SSE stream.

**Files:**
- `api_chat.py` â€” added `POST /api/chat/stop` endpoint that calls `task.cancel()` and emits a stopped event
- `frontend/src/api.ts` â€” added `stopAgentRun()` function
- `frontend/src/components/ChatView.tsx` â€” `handleStop()` now calls `stopAgentRun` before aborting the SSE connection

## 2026-02-20 â€” Prioritize quarterly reports over yearly when reading filings

**What:** Agent now defaults to the most recent quarterly report (Q3â†’midâ†’Q1) and only fetches the yearly report as a parallel companion, never alone.

**Files:**
- `config.py` â€” updated planning prompt routing rule with explicit priority order and "åˆ‡å‹¿å•ç‹¬è°ƒç”¨å¹´æŠ¥" constraint
- `tools/sina_reports.py` â€” updated `FETCH_COMPANY_REPORT_SCHEMA` description to reinforce the same rule at the tool level

## 2026-02-20 â€” Reduce Grok token cost for report summarization

**What:** Added `_prepare_for_grok` preprocessing step that eliminates ~50â€“64% of input tokens before sending to Grok, without losing any financial data.

**Files:**
- `tools/sina_reports.py` â€” added `_prepare_for_grok()`, updated `_grok_summarize_report()` to preprocess input and log reduction ratio

**Details:**
- Removes lines < 4 chars (page numbers, single-char separators, empty cells extracted as "-")
- Deduplicates lines â€” repeated table column headers, company name stamps, date labels account for ~34% of lines in typical Sina Finance reports
- If text still exceeds 80k chars after dedup (mainly å¹´æŠ¥), applies keyword-section extraction then hard cap
- Measured reductions: Q3 reports ~50%, å¹´æŠ¥ ~64% (216k â†’ 80k chars confirmed live)
- No capability loss: financial numbers only need to appear once for Grok to read them

## 2026-02-20 â€” Fix report routing and add Grok findings section

**What:** Agent now correctly routes "çœ‹/åˆ†æè´¢æŠ¥" queries to `fetch_company_report` instead of `web_search`; Grok's summary now includes a "å€¼å¾—å…³æ³¨çš„äº®ç‚¹æˆ–å¼‚å¸¸" section for downstream analysis.

**Files:**
- `config.py` â€” added routing rule to planning prompt: report queries â†’ `fetch_company_report`, never `web_search`
- `tools/sina_reports.py` â€” added section 6 to Grok summarization prompt: 2â€“4 notable findings with data citations

## 2026-02-20 â€” Use Grok to read and summarize full financial reports

**What:** `fetch_company_report` now feeds the full report HTML text to Grok (2M-token context) for AI-generated structured summaries, replacing the keyword-heuristic extraction.

**Files:**
- `tools/sina_reports.py` â€” added `_grok_client`, `_grok_summarize_report()`, updated `fetch_company_report()` to try Grok first with keyword-extraction fallback

**Details:**
- `_grok_summarize_report` sends the full scraped text + `focus_keywords` to `grok-4-1-fast-non-reasoning` via `chat.completions`
- Prompt instructs Grok to produce a Markdown report with tables covering: core metrics, balance sheet, cash flow, segment breakdown, risks
- `focus_keywords` (e.g. `['ä¸è‰¯ç‡', 'å‡€æ¯å·®', 'æ‹¨å¤‡è¦†ç›–ç‡']`) are included in the prompt so Grok pays special attention to them
- Return dict now includes `"summarized_by": "grok" | "keyword_extraction"` for transparency
- Falls back to `_extract_key_sections` if Grok is not configured or the call fails

## 2026-02-20 â€” Use Grok live search for web_search tool

**What:** Route `web_search` through Grok's built-in live search (xAI API) when `GROK_API_KEY` is set, with automatic DuckDuckGo fallback.

**Files:**
- `config.py` â€” added `GROK_API_KEY`, `GROK_BASE_URL`, `GROK_MODEL_NOREASONING`, `GROK_MODEL_REASONING`
- `tools/web.py` â€” added `_grok_client` (AsyncOpenAI pointing at xAI), `_grok_web_search()`, updated `web_search()` to prefer Grok

**Details:**
- xAI API is OpenAI-compatible; live search enabled via `extra_body={"search_parameters": {"mode": "auto"}}`
- Grok returns the answer in `choices[0].message.content` and citation URLs in a top-level `citations` list
- Falls back to DuckDuckGo silently if Grok is not configured or the call fails
- Uses `grok-4-1-fast-non-reasoning` model (configurable via `GROK_MODEL_noreasoning` env var)

## 2026-02-19 â€” Frontend reconnect for background agent runs

**What:** Added auto-reconnect logic so the UI reattaches to an in-progress agent run after a network drop or app backgrounding.

**Files:**
- `frontend/src/api.ts` â€” extracted `_readSSEStream` helper; added `fetchActiveRun` and `subscribeStream` exports
- `frontend/src/components/ChatView.tsx` â€” added `reconnect` callback, mount effect, and `visibilitychange` effect

**Details:**
- `fetchActiveRun(token)` calls `GET /api/chat/active` to check if an agent run is in progress for the user
- `subscribeStream(token, callbacks)` calls `GET /api/chat/stream` to replay buffered events and stream new ones; returns 404 â†’ `NO_ACTIVE_RUN` error handled silently
- `reconnect` guard: skips if already `sending`, skips if active run belongs to a different conversation
- Triggered on: initial mount (token available) and `document.visibilitychange â†’ visible`

## 2026-02-19 â€” Fix 400 bad request on invalid tool call JSON args

**What:** MiniMax occasionally returns tool calls with malformed JSON arguments. `_message_to_dict` was storing them raw into conversation history, causing a 400 on the next API call.

**Files:**
- `agent.py` â€” sanitize `tc.function.arguments` in `_message_to_dict` before adding to history

**Details:**
- `_execute_single_tool` already caught bad JSON (fell back to `{}`), but the raw bad string still got written into `msg_dict["tool_calls"]`
- Fix: `json.loads()` validation in `_message_to_dict`; replaces invalid args with `"{}"` and logs a warning

## 2026-02-19 â€” Lean system prompt + planning turn

**What:** Replaced the bloated routing-rule system prompt with a lean persona/style/citations prompt (~290 words) plus a dedicated `get_planning_prompt()`. Added a planning turn at the start of every agent run that forces intent resolution and tool mapping before any execution.

**Files:**
- `config.py` â€” rewrote `get_system_prompt()` (290 words, down from ~1700); added `get_planning_prompt()` with tool capability table and data availability notes
- `agent.py` â€” added planning turn before main loop; imports `get_planning_prompt`

**Details:**
- System prompt cut ~83% (1700 â†’ 290 words): removed tool priority list, all routing rules, efficiency rules, step-by-step examples â€” all moved into the planning prompt
- Planning turn: one tool-free LLM call that produces a research plan (intent, term resolution, tool mapping, data limits, parallel groups); plan injected into context + emitted as "Research Plan" thinking block
- Planning failures are non-fatal (logged, agent continues)
- Turn 0 label changed from "Turn 1 Â· Planning" to "Turn 1 Â· Analysis" since planning is now separate

## 2026-02-19 â€” Add å›½å®¶é˜Ÿ/èªæ˜é’± routing to system prompt

**What:** Added a dedicated routing block for "smart money" queries so the agent maps ambiguous terms (å›½å®¶é˜Ÿ, æ±‡é‡‘, è¯é‡‘) to the correct tools and data sources before calling anything.

**Files:**
- `config.py` â€” added "Smart money / èªæ˜é’± / å›½å®¶é˜Ÿ" routing section

**Details:**
- Root cause: agent had no routing for å›½å®¶é˜Ÿ â†’ fell back to web_search â†’ returned news fluff
- Fix: explicit lookup table mapping each term (åŒ—å‘èµ„é‡‘/å›½å®¶é˜Ÿ/ä¸»åŠ›) to its tool and data frequency
- å›½å®¶é˜Ÿ route: fetch_top_shareholders in parallel on 5 known ETFs + 4 large-cap bank stocks; look for ä¸­å¤®æ±‡é‡‘/è¯é‡‘å…¬å¸ in holder names and compare period changes
- Agent must state quarterly disclosure lag when reporting å›½å®¶é˜Ÿ data

## 2026-02-19 â€” Fix system prompt northbound flow description

**What:** Updated `config.py` system prompt to remove stale "discontinued" notes that caused the agent to skip calling `fetch_northbound_flow` entirely.

**Files:**
- `config.py` â€” updated tool description and routing rule for `fetch_northbound_flow`

**Details:**
- Old text said "net inflow data discontinued" â†’ agent concluded data unavailable and didn't call the tool
- New text reflects article-scraping capability and adds "ALWAYS call this tool"

## 2026-02-19 â€” Fix northbound flow via EastMoney article listing API

**What:** Replaced the broken `RPT_MUTUAL_DEAL_HISTORY` API with the EastMoney article listing API (column 399 = åŒ—å‘èµ„é‡‘åŠ¨æ€), which returns daily summaries with full structured data in each article's `summary` field.

**Files:**
- `tools/cn_capital_flow.py` â€” replaced implementation; single API call returns N days of data

**Details:**
- Discovered API endpoint from the page's embedded JS vars: `np-listapi.eastmoney.com/comm/web/getNewsByColumns?column=399&biz=web_stock&client=web&req_trace=nb`
- Each article `summary` contains: total åŒ—å‘ volume (äº¿), market share %, top-3 stocks for æ²ªè‚¡é€š + æ·±è‚¡é€š with exact amounts
- Parsed with regex into structured JSON; no per-article fetching needed
- Default `days=5`, max 30

## 2026-02-19 â€” Show per-turn agent thinking in regular conversations

**What:** Extended thinking block emission to every agent loop iteration (not just the final response), with per-turn unique sources and contextual labels so each thinking step shows as a separate collapsible block.

**Files:**
- `agent.py` â€” refactored think-extraction to run on every turn; added per-turn source IDs and labels

**Details:**
- Previously: `<think>` tags only extracted on the final no-tool-call turn; all used source `"agent"` so they merged
- Now: extracted from every LLM response before appending to conversation history
- Source is `agent_t{turn+1}` (unique per turn) so frontend shows them as separate blocks
- Labels: "Turn 1 Â· Planning", "Turn N Â· After {tool_names}", "Turn N Â· Synthesis"
- `<think>` content also stripped from `msg_dict["content"]` before it re-enters the message history, so the model doesn't see its own think tags on the next turn

## 2026-02-19 â€” Dynamic keyword extraction for fetch_company_report

**What:** Made `fetch_company_report` accept optional `focus_keywords` that the LLM passes based on the user's question, so report extraction adapts to any domain rather than relying solely on hardcoded markers.

**Files:**
- `tools/sina_reports.py` â€” added `focus_keywords` param to schema, `fetch_company_report`, and `_extract_key_sections`

**Details:**
- `focus_keywords` (optional array) is merged with base section markers at extraction time
- Tool description instructs the LLM to always derive and pass keywords from the user's question
- Also added bank-specific base markers (ä¸è‰¯ç‡, å‡€æ¯å·®, æ‹¨å¤‡è¦†ç›–ç‡, etc.) as a sensible default for bank stocks
- No extra planning round-trip needed â€” LLM already has user intent when calling the tool

## 2026-02-19 â€” Community sentiment via è‚¡å§ integrated into agent flows

**What:** Integrated è‚¡å§ community sentiment into both the debate system and the main agent's deep analysis workflow. Uses scrape_webpage on guba.eastmoney.com (no auth required). Xueqiu was tested and found to be WAF-blocked without session cookies, so dropped. Also fixed missing playwright/JS-domain definitions in web.py.

**Files:**
- `tools/trade_analyzer.py` â€” modified: added `_fetch_community_sentiment()` function; updated `run_hypothesis_debate` Phase 1 to run sentiment subagent in parallel with data collection for single_stock and comparison question types; sentiment section appended to data_pack before debaters receive it
- `tools/web.py` â€” fixed: added missing playwright try/except import, `PLAYWRIGHT_AVAILABLE` flag, and `_JS_HEAVY_DOMAINS` list (includes xueqiu.com, guba.eastmoney.com)
- `tools/__init__.py` â€” modified: removed fetch_eastmoney_forum and fetch_xueqiu_comments from TOOL_SCHEMAS and TOOL_MAP (tools are not exposed to the agent; sentiment handled internally)
- `config.py` â€” modified: updated forum routing to use scrape_webpage directly; updated deep analysis Step 1 to dispatch a sentiment subagent scraping guba; removed stale tool entries #25/#26 and their citation URLs

**Details:**
- `_fetch_community_sentiment` scrapes all stock entities in parallel, feeds combined text to a single MiniMax LLM call for summarization (~350 words output)
- Result is appended to the debate data_pack so all 4 debaters and the judge see retail sentiment alongside financial data
- For main agent (non-debate) deep analysis: dispatch_subagents handles guba scraping in parallel with financial data calls
- guba URL format: `https://guba.eastmoney.com/list,{6-digit-code}.html` â€” no SH/SZ prefix, no auth needed

## 2026-02-19 â€” Lift context window limits for 200k model + forum fallback

**What:** Raised all context/token limits across agents to better utilize a 200k-context MiniMax model; added web_search fallback when both forum sentiment tools fail.

**Files:**
- `agent.py` â€” modified: SUMMARIZE_THRESHOLD 30â†’60, SUMMARIZE_KEEP_RECENT 10â†’20, MAX_TOOL_RESULT_CHARS 15kâ†’40k, summarizer max_tokens 1500â†’2500
- `tools/subagent.py` â€” modified: added max_tokens=3000 to subagent LLM calls
- `tools/trade_analyzer.py` â€” modified: MAX_DEBATER_TOOL_RESULT_CHARS 12kâ†’25k, data_pack cap 60kâ†’100k chars, debater max_tokens 2000â†’4000, `_llm_call` default max_tokens 2000â†’3000 (judge/rebuttal)
- `config.py` â€” modified: forum routing fallback â€” if both fetch_eastmoney_forum and fetch_xueqiu_comments fail, fall back to web_search for retail sentiment

**Details:**
- Hypothesis formation stays at max_tokens=2000 (JSON output, no headroom needed)
- Summarizer fires less often (60 messages) but produces richer output when it does (2500 tokens)
- Debater increase to 4000 tokens ensures 800-word analyses with data citations are never truncated

## 2026-02-19 â€” Community sentiment tools (è‚¡å§ + é›ªçƒ)

**What:** Added two new tools for retail investor sentiment from Eastmoney è‚¡å§ and Xueqiu é›ªçƒ forums; wired them into the deep analysis workflow and system prompt routing.

**Files:**
- `tools/eastmoney_forum.py` â€” created: fetches and parses the SSR HTML post list from guba.eastmoney.com; returns titles, view/reply counts, authors, timestamps; falls back to link extraction if CSS selectors miss
- `tools/xueqiu.py` â€” already existed but was unregistered; no code changes
- `tools/__init__.py` â€” modified: imported and registered `fetch_xueqiu_comments` and `fetch_eastmoney_forum` in both `TOOL_SCHEMAS` and `TOOL_MAP`
- `config.py` â€” modified: replaced generic scrape_webpage forum guidance with dedicated tool routing; added both tools to the numbered tool list (#25, #26); added `fetch_eastmoney_forum` and `fetch_xueqiu_comments` to deep analysis Step 1 parallel calls; added citation URL mappings for both tools

**Details:**
- Eastmoney guba URL format: `https://guba.eastmoney.com/list,{code}.html` (page 1) / `list,{code}_{N}.html` (page N) â€” no SH/SZ prefix required
- Xueqiu API format: `https://xueqiu.com/query/v1/symbol/search/status?symbol=SH603986` â€” auto-detects SH vs SZ from the 6-digit code prefix

## 2026-02-19 â€” Full company reports in debate + revenue composition analysis

**What:** Removed report truncation so the agent reads entire annual reports. Added revenue composition and macro-sensitivity dimensions to debate prompts. Included `fetch_company_report` in the debate data plan so analysts see segment breakdowns, management discussion, and revenue structure.

**Files:**
- `tools/sina_reports.py` â€” modified: removed 8000 char cap in `_extract_key_sections`; raised per-section line limit from 60 to 150; added segment/revenue markers (åˆ†è¡Œä¸š, åˆ†äº§å“, æ”¶å…¥æ„æˆ, åˆ©æ¯å‡€æ”¶å…¥, ç»è¥æƒ…å†µè®¨è®ºä¸åˆ†æ, etc.)
- `tools/trade_analyzer.py` â€” modified: added `fetch_company_report` (yearly + mid) to single_stock and comparison data plan examples; updated hypothesis formation rules to mandate annual reports; raised data_pack cap from 30k to 60k chars; raised `MAX_DEBATER_TOOL_RESULT_CHARS` from 3k to 12k; added dimensions #1 (æ”¶å…¥ç»“æ„ä¸é©±åŠ¨åŠ›) and #2 (å®è§‚æ•æ„Ÿæ€§) to `_DIMENSIONS_SINGLE_STOCK` and `_DIMENSIONS_COMPARISON`
- `agent.py` â€” modified: raised `MAX_TOOL_RESULT_CHARS` from 4k to 15k so full reports reach the LLM

**Details:**
- Root cause: analysts never saw revenue breakdown because (a) `fetch_company_report` wasn't in the debate data plan, (b) report text was truncated to 8000 chars cutting off segment tables, (c) dimension prompts only asked about "revenue growth rate" not "where revenue comes from"
- New dimensions ask analysts to: identify revenue sources by segment/product/region, quantify each segment's growth, assess macro sensitivity (rate/FX/policy exposure), and project which business lines benefit or suffer under current conditions
- Truncation chain raised: tool output 4kâ†’15k, debater tool results 3kâ†’12k, data pack 30kâ†’60k, report extraction unlimited
- MiniMax-M1-80k has sufficient context window for these larger payloads

## 2026-02-19 â€” Per-user report management system + My Reports browser

**What:** Implemented per-user file isolation, authenticated file serving, persistent file tracking in the database, descriptive file naming, and a "My Reports" panel for browsing/filtering all generated files.

**Files:**
- `db.py` â€” modified: added `files` table with indexes on conversation_id, user_id, and filepath
- `accounts.py` â€” modified: added `save_file_record()`, `load_conversation_files()`, `load_user_files()` (with optional type filter + conversation title join)
- `agent.py` â€” modified: added `user_id_context` contextvar, set it around tool execution in both `_run_agent_inner` and `_run_debate_inner`, save file records to DB after tool results
- `tools/output.py` â€” modified: replaced static `OUTPUT_DIR` with `_get_output_dir()` that returns per-user subdirectory; added `_safe_filename()` for descriptive names based on title (e.g. `æ‹›å•†é“¶è¡Œåˆ†æ_20260219_a1b2.pdf`)
- `tools/trade_analyzer.py` â€” modified: same `_get_output_dir()` pattern replacing `_OUTPUT_DIR`
- `auth.py` â€” modified: added `get_current_user_or_query_token()` supporting both Authorization header and `?token=` query param (for `<img src>`)
- `api_chat.py` â€” modified: added `GET /api/chat/files` (list all user files with optional `?file_type=` filter); added `GET /api/chat/files/{filepath}` (authenticated serve with path traversal protection); updated `get_messages` to return `{messages, files}`; updated file URL generation to `/api/chat/files/` prefix
- `web.py` â€” modified: removed public `/output` static mount
- `frontend/src/api.ts` â€” modified: `fetchMessages` handles `{messages, files}` response; added `fetchUserFiles()`
- `frontend/src/components/ChatView.tsx` â€” modified: attaches conversation files to last assistant message on load
- `frontend/src/components/MessageBubble.tsx` â€” modified: images use `?token=` query param auth; PDFs/downloads use fetch + blob URL
- `frontend/src/components/ReportsPanel.tsx` â€” created: modal panel with type filter tabs (All/PDF/Charts/MD), search, thumbnail previews for images, click-to-download
- `frontend/src/components/Sidebar.tsx` â€” modified: added "My Reports" button + ReportsPanel toggle
- `frontend/src/i18n.tsx` â€” modified: added reports panel translation keys
- `frontend/src/styles/index.css` â€” modified: added reports button + reports panel styles

**Details:**
- Files table stores: user_id, conversation_id, filepath (relative), filename, file_type
- File ownership enforced at DB level â€” users can only access their own files
- Path traversal protection: resolved path must stay within PROJECT_ROOT
- Filenames now descriptive: `{sanitized_title}_{YYYYMMDD}_{4hex}.{ext}` instead of `report_{8hex}.pdf`
- My Reports panel reuses admin panel overlay/layout, adds search + type filter tabs
- Old flat `/output/` URLs no longer work â€” only new `/api/chat/files/` URLs are served

## 2026-02-18 â€” Match debate report language to user input

**What:** Fixed debate analyst outputs appearing in English when user asks in Chinese. Added dynamic `response_language` field to hypothesis and used it across all prompts to enforce consistent output language matching the user's input.

**Files:**
- `tools/trade_analyzer.py` â€” modified: added `response_language` to hypothesis schema; replaced ambiguous "write in the same language as the data" with explicit `{response_language}` in all 5 prompts (`_PRO_OPENING`, `_CON_OPENING`, `_REBUTTAL`, `_JUDGE`, `_SUMMARY`); translated dimension templates to Chinese

**Details:**
- Root cause: dimension headings were in English (e.g. "VALUATION", "EARNINGS TRAJECTORY"), and the language instruction was ambiguous, so some LLM analysts defaulted to English
- Fix: hypothesis formation now detects user language â†’ `response_language` field (e.g. "ä¸­æ–‡", "English") â†’ all prompts enforce output in that language
- Dimension templates (`_DIMENSIONS_SINGLE_STOCK`, `_DIMENSIONS_COMPARISON`, `_DIMENSIONS_SECTOR`, `_DIMENSIONS_GENERAL`) translated to Chinese

## 2026-02-18 â€” Separate debate mode from regular chat

**What:** Moved debate functionality out of the chat input area into a dedicated sidebar button with a modal dialog. Debate now creates a new conversation automatically.

**Files:**
- `frontend/src/components/ChatView.tsx` â€” modified: removed Debate button from input area, added `pendingDebate`/`onDebateStarted` props to auto-trigger debate from parent
- `frontend/src/pages/ChatLayout.tsx` â€” modified: added debate modal state, modal overlay with textarea, handles creating new conversation and passing pending debate to ChatView
- `frontend/src/components/Sidebar.tsx` â€” modified: added `onDebate` prop, added "Hypothesis Debate" button in sidebar header
- `frontend/src/styles/index.css` â€” modified: removed old `.input-area .debate-btn` styles, added sidebar debate button styles and debate modal overlay/dialog styles

**Details:**
- Debate button now lives in the sidebar header below "+ New Chat"
- Clicking it opens a centered modal with a textarea for the investment question
- Submitting creates a new conversation, sets it active, and auto-starts the debate via `pendingDebate` prop
- Enter submits, Shift+Enter for newline, Escape closes modal
- Modal auto-focuses the textarea on open

## 2026-02-18 â€” Fix PDF footer overlap, Chinese disclaimer, Songti font

**What:** Fixed PDF report generation: footer no longer overlaps body content, English disclaimer replaced with Chinese, font changed to Songti (å®‹ä½“) for better readability, body text spacing increased.

**Files:**
- `tools/output.py` â€” modified: added `_ReportPDF` FPDF subclass with proper `footer()` method (replaces buggy post-hoc page iteration); prioritized Songti.ttc in font search order; replaced English disclaimer with Chinese; increased body text to 10.5pt with 6.5 line height; added Unicode subscript sanitization (â‚€â†’0); changed bullet char from U+2022 to U+25CF (CJK-safe); increased auto page break margin to 30mm

**Details:**
- Root cause of overlap: old code looped through pages after content was done and wrote footer text, which collided with content on page boundaries
- Fix: `_ReportPDF.footer()` is called automatically by fpdf2 during page breaks, ensuring correct positioning
- Font priority: Songti.ttc (macOS) â†’ PingFang.ttc â†’ Noto Serif CJK (Linux) â†’ Noto Sans CJK â†’ fallback

## 2026-02-18 â€” Hypothesis-driven debate engine

**What:** Generalized the debate engine from hardcoded "is stock X worth investing in?" to handle any investment question (comparisons, sector analysis, general market questions) by forming a testable hypothesis (Hâ‚€) from the user's question, then having pro/con sides debate it.

**Files:**
- `tools/trade_analyzer.py` â€” modified: added `_form_hypothesis()` (Phase 0 LLM call to parse question into hypothesis + data plan), `_collect_data_from_plan()` (dynamic tool execution from plan), `run_hypothesis_debate()` (new main entry point); replaced `_BULL_OPENING`/`_BEAR_OPENING` with `_PRO_OPENING`/`_CON_OPENING`; generalized `_REBUTTAL`, `_JUDGE`, `_SUMMARY` prompts to use hypothesis framing; added dimension templates per question type; updated all phase functions to accept hypothesis dict; made `analyze_trade_opportunity()` a backward-compatible wrapper; updated report generation with hypothesis-aware titles and labels
- `agent.py` â€” modified: simplified `_run_debate_inner()` to pass user question directly to `run_hypothesis_debate()` (removed stock code extraction logic)
- `tools/__init__.py` â€” modified: added `run_hypothesis_debate` export
- `frontend/src/components/ChatView.tsx` â€” modified: updated default debate message to "Analyze the question discussed above"
- `changes.md` â€” modified: appended this entry

**Details:**
- Phase 0 forms hypothesis via LLM with 4 worked examples (single_stock, comparison, sector, general), full tool catalog, max 20 tool calls
- Data collection now executes arbitrary tool plans in parallel instead of hardcoded 7 tools
- Prompts use `{hypothesis}` and `{dimensions_text}` (per question type) instead of stock-specific framing
- Judge verdict options are dynamic from hypothesis (replaces hardcoded BUY/SELL/HOLD)
- Report filenames generated from entity names (e.g. "æ‹›å•†é“¶è¡Œ_vs_å·¥å•†é“¶è¡Œ_20260218_143000.md")
- Backward compatible: `analyze_trade_opportunity(stock_code="600036")` still works, internally calls `run_hypothesis_debate("600036 å€¼å¾—æŠ•èµ„å—?")`

## 2026-02-18 â€” Remove Gemini + fix billion/äº¿ unit conversion

**What:** Removed Gemini dependency (expensive), DuckDuckGo is now the sole search backend. Fixed critical unit conversion confusion where agents treated "billion" as "äº¿" (should be 10äº¿). Added explicit conversion rules to all 5 debate prompts and 3 system messages.

**Files:**
- `tools/web.py` â€” removed Gemini import, `_gemini_search_sync`, simplified `web_search` to DDG only
- `config.py` â€” removed `GEMINI_API_KEY`, updated web_search description
- `requirements.txt` â€” removed `google-genai`
- `tools/trade_analyzer.py` â€” added `_UNIT_RULE` constant with conversion examples, injected into all 5 prompts (_BULL_OPENING, _BEAR_OPENING, _REBUTTAL, _JUDGE, _SUMMARY) and 2 system messages

## 2026-02-18 â€” Fix Debate stock extraction + add Sina profit statement tool

**What:** Fixed critical bug where Debate button couldn't find the stock code (was only scanning assistant/tool messages, missing user messages where the stock name lives). Also added `fetch_sina_profit_statement` tool for structured annual profit data from Sina Finance.

**Files:**
- `agent.py` â€” fixed `_run_debate_inner`: now scans ALL message types (user/assistant/tool) for stock code; tries regex first (fast), falls back to LLM extraction
- `tools/sina_reports.py` â€” added `fetch_sina_profit_statement()` and `FETCH_SINA_PROFIT_SCHEMA`: scrapes `money.finance.sina.com.cn` profit statement tables by stock code and year
- `tools/__init__.py` â€” registered new tool in TOOL_SCHEMAS and TOOL_MAP
- `config.py` â€” added tool #11 description, citation URL mapping, renumbered tools 12-24

## 2026-02-18 â€” Add dedicated Debate button

**What:** Added a "Debate" button in the chat UI that directly invokes the multi-LLM trade analyzer without going through the agent loop. Extracts stock code from conversation context via a quick LLM call, passes gathered conversation data as context to avoid re-fetching. Works with or without typed input.

**Files:**
- `agent.py` â€” added `run_debate()` and `_run_debate_inner()`: extracts stock code from conversation, calls `analyze_trade_opportunity` directly with conversation context
- `api_chat.py` â€” added `mode` field to `SendBody`, routes `mode="debate"` to `run_debate()`
- `frontend/src/api.ts` â€” added `mode` parameter to `sendMessage()`
- `frontend/src/components/ChatView.tsx` â€” added Debate button, `handleDebate()`, refactored `handleSend()` to accept mode and override message
- `frontend/src/styles/index.css` â€” added `.debate-btn` styles (dark slate color)

## 2026-02-18 â€” Reference prior reports in trade analysis

**What:** Trade analyzer now checks the output/ directory for existing reports on the same stock (within 5 days). If found, the most recent report is included in the data pack as a soft reference â€” analysts can use data points and arguments from it but are instructed not to treat it as authoritative.

**Files:**
- `tools/trade_analyzer.py` â€” modified: added `_find_prior_report()` function, wired into `_collect_data()` with clear framing that prior reports are reference-only

## 2026-02-18 â€” Fix summary timeout + language matching

**What:** Fixed LLM timeout in executive summary phase and made output language match input language instead of hardcoding Chinese.

**Files:**
- `tools/trade_analyzer.py` â€” modified: increased `_run_summary` timeout to 120s / max_tokens to 3000; replaced all "Write in Chinese (ä¹¦é¢è¯­)" with language-matching instructions; added graceful fallback when summary LLM call fails (falls back to verdict instead of showing error string in report)

## 2026-02-17 â€” Archive Telegram bot, web-only startup

**What:** Moved Telegram bot component to `archive/` since the web app is the primary interface. Simplified `start.py` to web-only.

**Files:**
- `bot.py` â†’ `archive/bot.py` â€” moved
- `start.py` â€” modified: removed all Telegram bot logic, removed `TELEGRAM_BOT_TOKEN` import
- `config.py` â€” modified: removed `TELEGRAM_BOT_TOKEN` env var

**Details:**
- `start.py` now only runs the uvicorn web server (no more dual web+bot mode)
- Bot code preserved in `archive/` for reference if needed later

## 2026-02-17 â€” EastMoney structured data tools (financials, shareholders, dragon tiger, dividends)

**What:** Added 4 new tools using EastMoney datacenter APIs for structured financial data â€” financial statements, top shareholders, dragon tiger list, and dividend history. Updated system prompt to use structured APIs instead of web scraping for deep stock analysis.

**Files:**
- `tools/cn_eastmoney.py` â€” created: 4 tool functions (fetch_stock_financials, fetch_top_shareholders, fetch_dragon_tiger, fetch_dividend_history)
- `tools/__init__.py` â€” modified: registered 4 new tools + schemas (now 24 tools total)
- `config.py` â€” modified: added tools #14-17 to priority list, replaced old scrape-based routing for dividends/financials with direct API tools, updated deep analysis step to use structured financials + shareholders + capital flow + dividends in parallel (6 parallel calls), updated comparison routing, added citation URL mappings

**Details:**
- `fetch_stock_financials` â€” balance sheet, income statement, or cash flow. 10+ years of quarterly data from EastMoney datacenter API. Fields: revenue, net profit, YoY growth, assets, liabilities, debt ratio, operating/investing/financing cash flows.
- `fetch_top_shareholders` â€” top 10 circulating shareholders (åå¤§æµé€šè‚¡ä¸œ) with holding changes (æ–°è¿›/å¢æŒ/å‡æŒ/ä¸å˜). Grouped by reporting period.
- `fetch_dragon_tiger` â€” broker-level buy/sell data on exceptional trading days (æ¶¨è·Œåœ, æŒ¯å¹…>7%). Both buy-side and sell-side entries merged chronologically.
- `fetch_dividend_history` â€” complete dividend history with cash per 10 shares, bonus shares, ex-dividend dates, distribution progress, and EPS. Includes summary stats.
- Deep analysis now fetches 6 data sources in parallel: income statement (8 periods), balance sheet (4 periods), quote, capital flow (20 days), top shareholders (2 periods), dividend history.
- Old scraping-based routes for dividends/financials replaced with direct API calls.

## 2026-02-17 â€” Capital flow tools (èµ„é‡‘æµå‘)

**What:** Added 3 new tools for tracking institutional vs retail capital flow in Chinese A-shares, using EastMoney APIs.

**Files:**
- `tools/cn_capital_flow.py` â€” created: 3 tool functions (fetch_stock_capital_flow, fetch_northbound_flow, fetch_capital_flow_ranking)
- `tools/__init__.py` â€” modified: registered 3 new tools + schemas (now 20 tools total)
- `config.py` â€” modified: added tools to priority list (#11-13), added routing rules for capital flow queries, updated deep analysis rule to include capital flow, added citation URL mappings

**Details:**
- `fetch_stock_capital_flow` â€” daily capital flow for a single stock (~120 trading days / 6 months). Breaks down by order size: super-large (>100ä¸‡, institutional), large (20-100ä¸‡), medium (4-20ä¸‡), small (<4ä¸‡, retail). Includes period summary (total net, buy/sell day count).
- `fetch_northbound_flow` â€” Stock Connect (æ²ªæ·±æ¸¯é€š) daily deal volume. Note: net inflow/outflow data was discontinued after Aug 2024 due to regulatory changes. Deal amount and count still available.
- `fetch_capital_flow_ranking` â€” top stocks ranked by institutional net inflow or outflow. Shows which stocks institutions are buying/selling most heavily.
- System prompt updated: when user asks about a specific stock ("æœ€è¿‘æ€ä¹ˆæ ·", "analyze X"), agent now fetches capital flow alongside financial reports and quote data in parallel.
- All APIs are free, no auth, direct HTTP to EastMoney endpoints.

## 2026-02-17 â€” Code review: bug fixes and simplification

**What:** Fixed critical conversation routing bug, removed broken/dead code, consolidated duplicated code.

**Files:**
- `agent.py` â€” modified: added `conversation_id` parameter so web UI targets the correct conversation
- `api_chat.py` â€” modified: passes `conversation_id` through to `run_agent` instead of relying on "most recent" heuristic
- `db.py` â€” modified: made `init_db()` idempotent (skip if already initialized)
- `config.py` â€” modified: added `ADMIN_USERNAME` (moved from duplicated hardcoded values)
- `api_auth.py` â€” modified: import `ADMIN_USERNAME` from config, combined login into single pool acquire
- `api_admin.py` â€” modified: import `ADMIN_USERNAME` from config
- `tools/output.py` â€” modified: fixed `generate_references_image` KeyError (refs no longer have 'name' field)
- `tools/utils.py` â€” created: shared `safe_value()` function
- `tools/cn_market.py` â€” modified: use shared `safe_value`, removed duplicate definition
- `tools/cn_funds.py` â€” modified: use shared `safe_value`, removed duplicate definition
- `tools/market_scan.py` â€” modified: merged `_get_top_gainers`/`_get_top_losers` into single `_get_top_movers`
- `tools/sina_reports.py` â€” modified: removed dead code (line that was immediately overwritten)
- `start.py` â€” modified: removed unused `sys` import
- `cli.py` â€” deleted: was completely broken (wrong `run_agent` signature)

**Details:**
- Critical bug: web UI messages went to wrong conversation because `run_agent` always used "most recent" instead of the selected one
- `generate_references_image` would crash on Telegram with KeyError on `r['name']` since refs were simplified to URL-only
- `_safe_value` was duplicated across cn_market.py and cn_funds.py â€” moved to tools/utils.py
- `ADMIN_USERNAME` was hardcoded in two files â€” moved to config.py as env var
- `init_db()` was called twice when using start.py (once directly, once via web lifespan)

## 2026-02-17 â€” Professional PDF generation with Chinese font support

**What:** Rewrote PDF generation to render Chinese text correctly on Linux servers and produce professionally formatted reports.

**Files:**
- `tools/output.py` â€” modified: new CJK font resolution (macOS + Linux paths + auto-download fallback), professional layout with navy header band, color hierarchy, proper table rendering with borders/shading/alternating rows, page footers, bullet points, bold stripping
- `deploy.sh` â€” modified: added `fonts-wqy-microhei` to apt packages
- `.gitignore` â€” modified: added `fonts/` directory

**Details:**
- Font search order: bundled NotoSansSC â†’ macOS system fonts â†’ Linux packages (noto-cjk, wqy-microhei, wqy-zenhei) â†’ auto-download from Google Fonts as last resort
- PDF styling: dark navy header band with white title, steel blue section headings with underline rules, proper markdown table rendering with calculated column widths, alternating row colors, page number footers
- Tables handle cell overflow by truncating with ellipsis
- Long titles auto-wrap in header band
- Matplotlib Chinese font candidates also updated to include Linux fonts (Noto Sans CJK SC, WenQuanYi)

## 2026-02-15 â€” Comprehensive TradingView Scanner API field investigation

**What:** Tested 72+ TradingView Scanner API fields across 8 categories, documented all working/null fields, expanded the `screen_cn_stocks` tool with new columns, and rewrote `tradingview_data_sources.md`.

**Files:**
- `tradingview_data_sources.md` â€” rewritten with complete field catalog (54 verified working fields), filter recipes, industry-specific availability notes
- `tools/cn_screener.py` â€” expanded default columns (added ROA, margins, gross_profit, free_cash_flow, total_assets, total_debt, debt_to_equity, current_ratio, EPS, 52-week range, Perf.6M/Y), added `not_equal` filter op, updated schema descriptions
- `CLAUDE.md` â€” created with project guidelines for change logging

**Details:**
- 54 fields confirmed working for China A-shares, 24 fields always null (growth metrics, detailed balance sheet breakdowns, operating cash flow)
- Banks have sparser data (no gross_margin, ebitda); non-bank stocks return most fields
- Verified filters: PE < 10, mcap + PE + dividend combos, sector filtering, stock code lookup, `not_equal` operator
- New default columns give the agent much richer data per query without needing extra API calls

## 2026-02-16 â€” Footnote citations with reference image

**What:** Added mandatory footnote citations to all agent responses, with references rendered as a separate PNG image sent after the main reply in Telegram.

**Files:**
- `config.py` â€” added "Citations (MANDATORY)" section to system prompt instructing the LLM to always include `[references]...[/references]` blocks with numbered sources
- `tools/output.py` â€” added `parse_references()` to extract reference blocks from text, and `generate_references_image()` to render them as a dark-themed PNG with source names and URLs
- `bot.py` â€” integrated reference parsing: extracts references from agent response, strips the block from text, generates reference image, sends it as the last photo after any charts/PDFs

**Details:**
- References use `[references]...[/references]` delimiters with format `[N] Source Name | URL`
- Image uses dark theme (#1a1a2e background), blue source names, gray monospace URLs
- Supports Chinese text in source names, auto-wraps long URLs
- Reference image is always sent last (after charts and PDFs)
- If the LLM doesn't include references, no image is generated (graceful fallback)

## 2026-02-16 â€” Fix orphaned tool results in conversation history

**What:** Fixed `tool result's tool id not found` error caused by loading a conversation history window that starts mid-tool-call-sequence.

**Files:**
- `accounts.py` â€” added trimming logic to `load_recent_messages()` that skips orphaned `tool` and `assistant` (with tool_calls) messages from the front of the loaded window

**Details:**
- When loading the last N messages, the window could start with tool result messages whose parent assistant tool_call message was outside the window
- MiniMax API rejects these orphaned tool results with error 2013
- Fix: trim from the front until we hit a clean `user` or plain `assistant` message

## 2026-02-16 â€” Web UI for Financial Research Agent

**What:** Added a browser-based chat interface (FastAPI backend + React frontend) so the agent is accessible from anywhere, not just Telegram.

**Files:**
- `config.py` â€” modified: added `JWT_SECRET`, `WEB_PORT` env vars
- `db.py` â€” modified: added `web_accounts` table to SCHEMA_SQL
- `requirements.txt` â€” modified: added `fastapi`, `uvicorn[standard]`, `PyJWT`, `bcrypt`, `python-multipart`
- `auth.py` â€” created: bcrypt password hashing + JWT token creation/validation
- `api_auth.py` â€” created: `/api/auth/register`, `/api/auth/login`, `/api/auth/me` endpoints
- `api_chat.py` â€” created: conversations CRUD + SSE `/api/chat/send` endpoint integrating with `run_agent()`
- `web.py` â€” created: FastAPI app entry point with CORS, static files, lifespan DB init
- `frontend/` â€” created: React + Vite + TypeScript SPA
  - `src/api.ts` â€” HTTP client + SSE POST stream parser
  - `src/store.tsx` â€” AuthContext with localStorage token persistence
  - `src/App.tsx` â€” Router with protected routes
  - `src/pages/LoginPage.tsx` â€” Login/register form
  - `src/pages/ChatLayout.tsx` â€” Sidebar + chat main layout
  - `src/components/ChatView.tsx` â€” Message list + input + SSE integration
  - `src/components/MessageBubble.tsx` â€” Markdown rendering with react-markdown + remark-gfm
  - `src/components/Sidebar.tsx` â€” Conversation list + new chat + delete
  - `src/components/ReferenceCard.tsx` â€” Styled reference links (HTML, not PNG)
  - `src/components/StatusIndicator.tsx` â€” Animated "Thinking..." / "Running: tool" dots
  - `src/styles/index.css` â€” Dark theme (#0f0f23 bg, #82b1ff accent), responsive mobile
- `.gitignore` â€” modified: added `frontend/node_modules/`, `frontend/dist/`

**Details:**
- SSE streaming from POST using `fetch()` + `ReadableStream` (not EventSource which is GET-only)
- Auto-sets conversation title from first user message
- References rendered as styled HTML card (not PNG image like Telegram)
- Responsive design: mobile sidebar collapses into hamburger menu overlay
- Token expiry returns 401 which triggers automatic redirect to login
- Telegram bot continues running independently alongside the web server
- Production: `cd frontend && npm run build` then `uvicorn web:app` serves everything on one port

## 2026-02-16 â€” Fix references display + Sina Finance report tool

**What:** Fixed references not showing in web UI for historical messages, and added a new `fetch_company_report` tool that scrapes Sina Finance for company financial reports (å¹´æŠ¥/å­£æŠ¥). Updated system prompt to always fetch reports when analyzing a specific company.

**Files:**
- `frontend/src/components/ChatView.tsx` â€” modified: added client-side `parseReferences()` to extract references from historical message content when loading conversations
- `tools/sina_reports.py` â€” created: `fetch_company_report` tool that scrapes Sina Finance bulletin pages (`vCB_Bulletin`, `vCB_BulletinYi/Zhong/San`) to find the latest report, extracts key financial sections and PDF link
- `tools/__init__.py` â€” modified: registered `fetch_company_report` + `FETCH_COMPANY_REPORT_SCHEMA` (now 17 tools)
- `config.py` â€” modified: added `fetch_company_report` to tool priority list (#10), added mandatory routing rule for company analysis (always fetch yearly + latest quarterly report in parallel), added citation URL mapping

**Details:**
- References fix: when loading historical messages, the raw `[references]...[/references]` block was still in the DB content but wasn't being parsed. Added frontend-side regex parsing matching the Python `parse_references()` logic
- Sina report URLs: yearly=`vCB_Bulletin`, Q1=`vCB_BulletinYi`, mid=`vCB_BulletinZhong`, Q3=`vCB_BulletinSan`, all under `vip.stock.finance.sina.com.cn`
- Report tool extracts key financial sections (ä¸»è¦è´¢åŠ¡æ•°æ®, è¥ä¸šæ”¶å…¥, å‡€åˆ©æ¶¦, èµ„äº§è´Ÿå€º, ç°é‡‘æµ, åˆ†çº¢, etc.) and limits to ~8000 chars
- Also extracts PDF download link from report detail page
- System prompt now mandates: when user asks about a specific company, call `fetch_company_report(yearly)` + `fetch_company_report(latest quarter)` + `fetch_cn_stock_data(quote)` in parallel

## 2026-02-16 â€” Single-command startup + parchment light theme

**What:** Created `start.py` to launch the entire app (web server + Telegram bot) with one command. Redesigned the web UI from dark theme to a warm parchment/light color scheme.

**Files:**
- `start.py` â€” created: single entry point that auto-builds frontend if stale, runs uvicorn web server + Telegram bot concurrently via asyncio
- `frontend/src/styles/index.css` â€” rewritten: warm parchment palette (#f5f0e6 bg, #ebe5d7 sidebar, #8b6914 accent), serif font (Georgia/Songti SC), white input field, light assistant bubbles with border

**Details:**
- `python start.py` does everything: installs npm deps if needed, builds frontend if dist/ is stale, inits DB, starts web server on configured port, starts Telegram bot if token is set
- If TELEGRAM_BOT_TOKEN is not set, only the web server runs (no error)
- Frontend auto-builds only when src/ files are newer than dist/
- Parchment theme: warm off-white backgrounds, dark brown text, golden-brown accent (#8b6914), serif typography for a document/research feel

## 2026-02-17 â€” Research: free APIs for China A-share order flow / capital flow data

**What:** Investigated 6 data sources for buyer/seller order flow (ä¹°å–ç›˜, èµ„é‡‘æµå‘, ä¸»åŠ›èµ„é‡‘) for China A-share stocks, documenting endpoints, data returned, and authentication requirements.

**Files:**
- `changes.md` â€” modified (this entry)

**Details:**
- Best option for integration: EastMoney push2 direct HTTP API (no auth, JSON, real-time capital flow by order size) or AKShare library (wraps EastMoney/Sina, pip install, no API key)
- Sina Finance hq.sinajs.cn provides free 5-level order book but requires Referer header spoofing
- TradingView Scanner has no capital flow fields for China A-shares
- Tushare requires registration + API token (not truly free/anonymous)
- Full details in research summary provided to user

## 2026-02-17 â€” Comprehensive EastMoney data sub-pages API endpoint investigation

**What:** Systematically investigated all 20 data sub-pages on EastMoney for individual Chinese A-share stocks, testing API endpoints to find working JSON data sources. Created comprehensive documentation of 14 verified working endpoints across 6 categories.

**Files:**
- `EASTMONEY_API_ENDPOINTS.md` â€” created: Complete API documentation with endpoint URLs, parameters, response formats, field descriptions, and usage examples

**Details:**
- Tested stock code 600173 (å§é¾™æ–°èƒ½) across 20 different data categories
- Successfully identified 14 working API endpoints:
  - Market Data (5 endpoints): Real-time quote, intraday tick data, K-line/OHLC, main force capital flow, stock fund flow details
  - Trading Intelligence (2 endpoints): Dragon Tiger List buy/sell sides (å¼‚åŠ¨è¥ä¸šéƒ¨äº¤æ˜“)
  - Financial Statements (3 endpoints): Balance sheet, income statement, cash flow statement
  - Shareholding (2 endpoints): Top 10 circulating shareholders, top 10 total shareholders
  - Corporate Actions (1 endpoint): Dividends and share bonus distributions
  - Company Info (1 endpoint): Core themes/concepts classification
- Two API types discovered:
  - `datacenter-web.eastmoney.com/api/data/v1/get` - Structured financial/corporate data with report names
  - `push2.eastmoney.com` / `push2his.eastmoney.com` - Real-time and historical market data
- Financial statements provide 10+ years of quarterly/annual data (104 records for balance sheet)
- Dragon Tiger List tracks major institutional broker buy/sell transactions on exceptional trading days (412+ records)
- Shareholder data shows top 10 holders with holding changes across multiple reporting periods (920+ records)
- Documentation includes: field counts, sample data, value ratings (HIGH/MEDIUM), Python usage examples, stock code format specifications
- Missing/unavailable: 15 data types could not be found via API (announcements, stock calendar, margin trading, block trades, executive holdings, research reports, etc.) - may require scraping or authentication
- Recommended priority: 8 HIGH-value endpoints identified for immediate agent tool integration

## 2026-02-17 â€” Multi-LLM debate system for trade opportunity analysis

**What:** Added `analyze_trade_opportunity` tool â€” a structured 4-phase debate system using MiniMax + Qwen to produce buy/sell/hold verdicts with anonymized judging.

**Files:**
- `tools/trade_analyzer.py` â€” created: multi-LLM debate orchestrator (~440 lines) with 4 phases: data collection, opening arguments, rebuttals, anonymized judge
- `tools/__init__.py` â€” modified: registered `analyze_trade_opportunity` + `ANALYZE_TRADE_SCHEMA` (now 25 tools total)
- `config.py` â€” modified: added `QWEN_API_KEY`, `QWEN_BASE_URL`, `QWEN_MODEL` env vars; added tool #22 to priority list; added routing rule for "å€¼å¾—ä¹°å—"/"should I buy" triggers; added citation URL mapping

**Details:**
- Phase 1: Parallel data collection via 7 existing tools (income/balance/cashflow statements, quote, capital flow, shareholders, dividends)
- Phase 2: 4 parallel LLM calls â€” Bull-A (MiniMax), Bull-B (Qwen), Bear-A (MiniMax), Bear-B (Qwen) with structured prompts covering 8 mandatory analysis dimensions
- Phase 3: 4 parallel rebuttal calls â€” each debater sees opposing arguments + ally's argument, produces targeted counter-arguments
- Phase 4: 1 MiniMax judge call â€” all 8 arguments shuffled randomly, labeled Analyst 1-8 (no model attribution), produces verdict with confidence score, rationale, risks, dissenting view, time horizon
- Circular import avoided via late import of `execute_tool` in `_execute_tool` wrapper
- Total: ~9 LLM calls + 7 data tool calls per analysis, ~30-60 seconds end-to-end

## 2026-02-17 â€” Research: TradingAgents multi-agent debate architecture analysis

**What:** Deep analysis of TauricResearch/TradingAgents GitHub repo's multi-agent debate system architecture, covering all agent roles, prompts, debate flow, state management, memory/reflection, and final decision pipeline.

**Files:**
- `changes.md` â€” modified (this entry)

**Details:**
- Fetched and analyzed 16 source files from the TradingAgents repo covering analysts, researchers, trader, risk management debaters, managers, graph orchestration, conditional logic, reflection, propagation, signal processing, and agent state definitions
- Documented exact prompts for all 10+ agent roles, two-tier debate structure (bull/bear investment debate + 3-way risk debate), state passing via LangGraph TypedDict, memory/reflection learning loop, and signal extraction pipeline
- Key finding: system uses configurable debate rounds (default 1 round each) with round-robin turn-taking controlled by counter-based conditional edges in a LangGraph StateGraph

## 2026-02-18 â€” Show agent thinking process with model name in frontend

**What:** Added real-time status updates showing which model is working and what phase the trade analyzer is in, replacing generic "Thinking..." with model-tagged progress.

**Files:**
- `agent.py` â€” modified: added `contextvars.ContextVar` for status callback, changed status to "MiniMax Â· Thinking...", set/reset contextvar around tool execution
- `tools/trade_analyzer.py` â€” modified: reads status callback contextvar, emits phase-specific status ("Collecting market data...", "MiniMax + Qwen Â· Opening arguments...", etc.)
- `frontend/src/components/StatusIndicator.tsx` â€” modified: parses " Â· " separator to render model name as styled badge pill
- `frontend/src/styles/index.css` â€” modified: added `.status-model` pill styling (accent-colored badge)

**Details:**
- Status progression during trade analysis: "Collecting market data..." â†’ "MiniMax + Qwen Â· Opening arguments (4 analysts)..." â†’ "MiniMax + Qwen Â· Rebuttals (4 analysts)..." â†’ "MiniMax Â· Judge rendering verdict..."
- Uses `contextvars.ContextVar` to pass the status callback to tools without changing the `execute_tool` interface
- Frontend parsing is backwards-compatible: status text without " Â· " renders as before

## 2026-02-18 â€” Tool-augmented debaters + thinking display

**What:** Debaters in the trade analyzer now have access to research tools (web search, financial data) to strengthen arguments with live evidence. All `<think>` blocks from MiniMax are extracted and displayed as collapsible reasoning blocks in the frontend.

**Files:**
- `agent.py` â€” modified: added `thinking_callback` contextvar, `on_thinking` param to `run_agent`/`_run_agent_inner`, extract `<think>` content before stripping, set both contextvars around tool execution
- `tools/trade_analyzer.py` â€” modified: added `_llm_call_with_tools` mini agent loop (max 3 tool rounds, 90s timeout), `_get_debater_tool_schemas` (excludes output/meta/recursive tools), `_msg_to_dict`, `_truncate_tool_result` (3000 char cap), `_extract_and_strip_thinking`, thinking extraction in `_llm_call` (judge), updated prompts with tool-access instruction, wired `status_fn`/`thinking_fn` through all phases
- `api_chat.py` â€” modified: added `on_thinking` callback that queues `{"event": "thinking", ...}` SSE events, passes to `run_agent`
- `frontend/src/api.ts` â€” modified: added `onThinking` to `SSECallbacks`, parses `thinking` SSE event type
- `frontend/src/components/ThinkingBlock.tsx` â€” created: collapsible block with arrow toggle, italic label, pre-wrapped content area (max 300px scroll), border-left accent
- `frontend/src/components/ChatView.tsx` â€” modified: `thinkingBlocks` state + ref for stale closure safety, `onThinking` merges by source, attaches accumulated blocks to assistant message on done, renders in-progress blocks above status
- `frontend/src/components/MessageBubble.tsx` â€” modified: accepts `thinking` prop, renders `ThinkingBlock` components above message content
- `frontend/src/styles/index.css` â€” modified: added `.thinking-blocks`, `.thinking-block`, `.thinking-toggle`, `.thinking-arrow`, `.thinking-label`, `.thinking-content` styles

**Details:**
- Debaters use all data-fetching tools (19 tools) except generate_chart, generate_pdf, dispatch_subagents, analyze_trade_opportunity, lookup_data_sources, save_data_source
- Max 3 tool rounds per debater, then forced text-only on round 4
- Status shows individual tool calls: "Bull Analyst A (MiniMax) Â· Searching: web_search..."
- Judge stays on plain `_llm_call` with no tool access
- Thinking blocks appear in real-time during streaming, collapse when message finalizes
- Same-source thinking content is appended (merged) to avoid duplicate blocks

## 2026-02-18 â€” Streaming thinking display + auto MD/PDF reports for trade analysis

**What:** Replaced collapsible thinking blocks with a subtle streaming text display that flows smoothly during processing. Added automatic markdown + PDF report generation for every trade analysis, named `{stock_name}_{timestamp}.md/.pdf`.

**Files:**
- `frontend/src/components/ThinkingBlock.tsx` â€” rewritten: subtle streaming text with auto-scroll, muted opacity, click-to-expand/collapse, `streaming` prop for live mode
- `frontend/src/styles/index.css` â€” modified: replaced heavy `.thinking-block` styles with subtle `.thinking-stream` styles (low opacity, no borders, muted text)
- `frontend/src/components/ChatView.tsx` â€” modified: passes `streaming={true}` to in-progress thinking blocks
- `tools/trade_analyzer.py` â€” modified: added `_build_report_markdown` (structures debate into sections), `_generate_report` (saves MD + generates PDF via existing `generate_pdf`), returns `files` list in result, new Phase 5 after judge
- `agent.py` â€” modified: handles `result["files"]` (list) in addition to `result["file"]` (single) when extracting file paths from tool results

**Details:**
- Thinking text streams in at 55% opacity during processing, 70% while active streaming, rises to 80% on hover
- Historical thinking on completed messages starts collapsed, click header to expand
- Reports saved as `{stock_name}_{YYYYMMDD_HHMMSS}.md` and `.pdf` in `output/` dir
- MD report includes: verdict, all 4 opening arguments, all 4 rebuttals, data summary (first 5000 chars)
- PDF generated using existing `generate_pdf` tool then renamed to match naming convention
- Both files served via `/output/` static mount and appear as download links in chat

## 2026-02-18 â€” Stop generation button

**What:** Added ability to stop the AI mid-response and correct your message. Textarea stays enabled during generation.

**Files:**
- `frontend/src/components/ChatView.tsx` â€” modified: added `handleStop` (aborts SSE, removes optimistic user message, resets state), Send button swaps to red Stop button while sending, Enter while sending stops generation, textarea no longer disabled during sending
- `frontend/src/styles/index.css` â€” modified: added `.stop-btn` styles (red background)

**Details:**
- Stop aborts the SSE fetch, clears thinking blocks and status, removes the pending user message so the user can retype
- Textarea stays active during generation so the user can type their correction while waiting
- Enter during generation = stop; Enter again = send the corrected message
- Stop button uses `var(--error)` color to distinguish from Send

## 2026-02-18 â€” Data-driven debate prompts (remove emotional tone)

**What:** Rewrote all debate prompts and system messages to enforce strictly quantitative, data-only analysis. Eliminated advocacy framing, combative language, and emotional adjectives.

**Files:**
- `tools/trade_analyzer.py` â€” modified: rewrote `_BULL_OPENING`, `_BEAR_OPENING`, `_REBUTTAL`, `_JUDGE` prompts and all 3 system messages

**Details:**
- Bull/bear analysts reframed as "quantitative equity/risk analyst" instead of "buy-side/risk analyst building a case"
- Explicit ban on subjective adjectives: "ç¦æ­¢ä½¿ç”¨ä¸»è§‚å½¢å®¹è¯" â€” no "å¼ºåŠ²", "ä¼˜ç§€", "ä»¤äººæ‹…å¿§", "ä¸¥é‡"
- Every claim must include a specific number or it's invalid
- Rebuttals reframed from "dismantle their points" to "examine data accuracy and completeness"
- Anti-combative language rule: no "ä»–ä»¬å¿½ç•¥äº†"/"è¿™æ˜¯é”™è¯¯çš„", instead "è¯¥æ•°æ®ç‚¹éœ€è¡¥å……èƒŒæ™¯: [å…·ä½“æ•°æ®]"
- Both sides must acknowledge when opposing data is correct â€” no spinning
- Judge evaluates data accuracy/completeness, explicitly told to "disregard emotional language, rhetorical flourish, unsubstantiated predictions"
- System messages changed from "ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆ" to "é‡åŒ–é‡‘èåˆ†æå¸ˆ" with "ä»…åŸºäºæ•°æ®åˆ†æ" constraint

## 2026-02-18 â€” Executive summary phase + institutional-quality reports

**What:** Added Phase 5 (executive summary LLM call) after the judge verdict to synthesize the entire debate into a structured, fact-only summary. Rewrote report generation to produce institutional-standard MD/PDF with proper structure.

**Files:**
- `tools/trade_analyzer.py` â€” modified: added `_SUMMARY` prompt, `_run_summary` function (Phase 5), rewrote `_build_report_markdown` (no raw JSON, proper sections: exec summary â†’ verdict â†’ appendix with full arguments), updated `_generate_report` and `analyze_trade_opportunity` to include summary
- `tools/output.py` â€” modified: PDF renderer now handles `---` horizontal rules, numbered lists (`1. ...`), disclaimer footer on every page

**Details:**
- New Phase 5: executive summary LLM call produces structured output with: æ‰§è¡Œæ‘˜è¦, å…³é”®è´¢åŠ¡æŒ‡æ ‡ table, å¤šæ–¹/ç©ºæ–¹æ ¸å¿ƒè®ºæ®, äº‰è®®ç„¦ç‚¹ä¸æ•°æ®åˆ†æ­§, é£é™©å› ç´ , ç»“è®ºä¸å»ºè®®
- Summary prompt enforces: every bullet must contain a specific number, no adjectives, 800-1200 words
- Report structure: exec summary up front (what a PM reads), verdict second, full debate in numbered appendix (A.1-A.8)
- Removed raw JSON data dump from report â€” data now lives only in the summary's key metrics table
- PDF footer: "AI-generated report. For reference only. Not investment advice." + page numbers
- Pipeline is now 6 phases: data collection â†’ openings â†’ rebuttals â†’ judge â†’ summary â†’ report generation

## 2026-02-18 â€” UI Language Toggle (English / ä¸­æ–‡)

**What:** Added a lightweight i18n system with a language toggle so users can switch the UI between English and Chinese.

**Files:**
- `frontend/src/i18n.tsx` â€” created: translations dict (~40 keys), `LanguageProvider` context, `useT()` hook, defaults to Chinese, persists choice in localStorage
- `frontend/src/main.tsx` â€” modified: wrapped app with `<LanguageProvider>`
- `frontend/src/pages/LoginPage.tsx` â€” modified: replaced hardcoded strings with `t(...)` calls
- `frontend/src/pages/ChatLayout.tsx` â€” modified: replaced debate modal strings with `t(...)`
- `frontend/src/components/Sidebar.tsx` â€” modified: replaced strings with `t(...)`, added language toggle button
- `frontend/src/components/ChatView.tsx` â€” modified: replaced strings with `t(...)`
- `frontend/src/components/AdminPanel.tsx` â€” modified: replaced strings with `t(...)`
- `frontend/src/components/ThinkingBlock.tsx` â€” modified: replaced "show"/"hide" with `t(...)`
- `frontend/src/components/ReferenceCard.tsx` â€” modified: replaced "References" with `t(...)`
- `frontend/src/styles/index.css` â€” modified: added `.lang-toggle` button styles

**Details:**
- No external i18n library â€” custom React context with ~40 translation keys for 2 languages
- Default language is Chinese (`zh`), persisted in `localStorage` under key `lang`
- Toggle button in sidebar footer shows "EN" when in Chinese mode, "ä¸­" when in English mode
- All user-facing strings translated: login, sidebar, chat, debate modal, thinking blocks, references, admin panel

## 2026-02-18 â€” Fix conversation isolation + add conversation summarization

**What:** Fixed multiple state management bugs causing conversations to interfere under concurrent load. Added automatic conversation summarization for long conversations.

**Files:**
- `tools/cache.py` â€” modified: added `asyncio.Lock` to protect global `_cache` dict mutations; `set_cached` is now async; `get_cached` no longer mutates dict without lock
- `accounts.py` â€” modified: `_user_locks` now stores `(lock, last_used_time)` tuples with TTL-based cleanup (1hr); added `get_conversation_summary()`, `save_conversation_summary()`, `load_messages_for_summarization()` for summarization support
- `tools/sources.py` â€” modified: added `fcntl` file locking (`LOCK_SH` for reads, `LOCK_EX` for writes) to prevent concurrent write corruption on `sources.json`
- `db.py` â€” modified: added `summary` (TEXT) and `summary_up_to` (INTEGER) columns to conversations table for persisting conversation summaries
- `agent.py` â€” modified: added `_maybe_summarize()` function and `_SUMMARIZE_PROMPT`; wired summarization into `_run_agent_inner` after loading messages and before prepending system prompt; imports new accounts functions

**Details:**
- **Cache fix**: The global `_cache` dict was shared across all concurrent requests. Dict mutations during `_evict_expired()` and `set_cached()` could corrupt under concurrent access. Now protected by `asyncio.Lock`. Cache keys remain user-agnostic (market data is the same for all users) â€” this is intentional.
- **User locks fix**: `_user_locks` dict grew unbounded. Now each entry tracks last-used time, and stale locks (unused >1hr, not currently held) are cleaned up every 5 minutes.
- **Sources fix**: `_load_sources`/`_save_sources` had a classic read-modify-write race condition. Two concurrent `save_data_source` calls could lose one write. Now uses `fcntl.flock` for exclusive write locking and shared read locking.
- **Conversation summarization**: When a conversation exceeds 30 user+assistant messages, older messages are summarized into a single dense paragraph via an LLM call. The summary is persisted in the `conversations.summary` column and prepended to the message list on future requests. Recent 10 messages are always kept unsummarized. Full history remains in the `messages` table untouched. Follows the same pattern as LangGraph checkpointers and OpenAI's conversation state API.

## 2026-02-20 â€” Add report_cache table schema

**What:** Added the `report_cache` table to `SCHEMA_SQL` in `db.py` to cache distilled financial report metadata and file paths.

**Files:**
- `db.py` â€” modified (appended `report_cache` table + lookup index to `SCHEMA_SQL`)

**Details:**
- Table stores one row per (stock_code, report_type, report_year) with a UNIQUE constraint on that triple
- Columns: `id`, `stock_code` (CHAR 6), `report_type` (yearly/q1/mid/q3), `report_year` (SMALLINT), `report_date` (filing date string), `title`, `filepath` (relative path to distilled .md), `source_url`, `created_at`
- `idx_report_cache_lookup` index added on (stock_code, report_type, report_year) for fast lookups
- No migration tooling needed; `init_db()` applies the schema via `CREATE TABLE IF NOT EXISTS` on next server start

## 2026-02-20 â€” TOC parser and chapter classifier for CN annual reports

**What:** Added constants, regex patterns, `_should_keep_chapter()`, and `_parse_toc()` to `tools/sina_reports.py` so callers can filter boilerplate sections from Chinese annual reports before sending text to an LLM.

**Files:**
- `tools/sina_reports.py` â€” modified (added constants and two functions after `_extract_key_sections`)

**Details:**
- `_KEEP_CHAPTER_KEYWORDS` and `_SKIP_CHAPTER_KEYWORDS` drive classification; keep-keywords are checked first so mixed-title chapters like "å…¬å¸ç®€ä»‹å’Œä¸»è¦è´¢åŠ¡æŒ‡æ ‡" are correctly retained
- `_TOC_ENTRY_RE` matches TOC lines of the form "ç¬¬NèŠ‚ Title ...... pagenum"
- `_CHAPTER_HEADING_RE` matches chapter headings in body text (available for future use)
- `_parse_toc()` scans the first 400 lines; returns `[]` when no TOC is detected (callers treat that as "no filter")
- Unknown chapters default to keep (True) to avoid accidental data loss

## 2026-02-20 â€” Add TOC-based section filter for report pre-processing

**What:** Added `_filter_sections_by_toc()` to `tools/sina_reports.py` to physically remove skip-chapters from report text using TOC-detected chapter boundaries.

**Files:**
- `tools/sina_reports.py` â€” modified (added `_filter_sections_by_toc` immediately after `_parse_toc`)

**Details:**
- Scans body lines from position 50 onwards to skip the TOC block itself
- Matches chapter headings via `_CHAPTER_HEADING_RE` and looks up keep/skip status from the parsed chapters list using the first 6 chars of each chapter name
- Collects kept chapter blocks into a result list; falls back to full original text if no boundaries found or if input is large (>10000 chars) and filtered output is suspiciously small (<1000 chars)
- Safety threshold uses `len(text) > 10000 and len(filtered) < 1000` so small/test inputs are not incorrectly fallen back

## 2026-02-20 â€” Integrate TOC-based pre-filter into _prepare_for_grok

**What:** Wired `_parse_toc()` and `_filter_sections_by_toc()` as Step 1 in `_prepare_for_grok()`, so skip-chapters (é‡è¦æç¤º, å…¬å¸æ²»ç†, ç¯å¢ƒç¤¾ä¼šè´£ä»», etc.) are dropped before deduplication and keyword extraction.

**Files:**
- `tools/sina_reports.py` â€” modified (_prepare_for_grok body replaced)

**Details:**
- New Step 1 calls `_parse_toc(full_text)`; if chapters are found, calls `_filter_sections_by_toc()` and logs the character reduction percentage and chapter count
- If no TOC is detected, falls back to full text with an info log message
- Step 2 (deduplication) and Step 3 (keyword-section extraction + hard cap) are unchanged in logic, but now operate on already-filtered text
- TOC filtering typically removes 40â€“60% of annual report text before the remaining steps run

## 2026-02-20 â€” Improve Grok prompt for structured Option-B report distillation

**What:** Replaced `_grok_summarize_report()` in `tools/sina_reports.py` with an Option-B prompt that produces both a full narrative and preserved financial tables as Markdown tables.

**Files:**
- `tools/sina_reports.py` â€” modified (replaced `_grok_summarize_report` function body)

**Details:**
- New docstring clarifies the function returns a structured Markdown string (Option B: narrative + tables).
- System prompt now explicitly instructs Grok to preserve all financial tables in Markdown table format and not omit any figures or ratios.
- User prompt restructured into five labelled sections: æ ¸å¿ƒè´¢åŠ¡æŒ‡æ ‡ (with a table), ç®¡ç†å±‚è®¨è®ºä¸åˆ†ææ‘˜è¦ (including segmented revenue tables), è´¢åŠ¡æŠ¥è¡¨å…³é”®æ•°æ® (P&L, balance sheet, cash flow, industry-specific KPIs), é£é™©å› ç´ , and äº®ç‚¹ä¸å¼‚å¸¸å‘ç°.
- Added a pre-processing context block ("é˜…è¯»ç­–ç•¥") explaining which report sections were retained vs. filtered.
- `focus_note` label changed from "è¯·ç‰¹åˆ«å…³æ³¨ä»¥ä¸‹æŒ‡æ ‡" to "**é‡ç‚¹å…³æ³¨æŒ‡æ ‡**" for Markdown emphasis.
- Division-by-zero guard added: `max(len(full_text), 1)` in the reduction-percentage log line.

## 2026-02-20 â€” Add cache path helper and report year extraction

**What:** Added `_get_cache_path()` and `_extract_report_year()` helper functions to `tools/sina_reports.py`, along with the `_REPORTS_BASE` path constant and `pathlib.Path` import, to support future report caching.

**Files:**
- `tools/sina_reports.py` â€” modified (added `from pathlib import Path`, `_REPORTS_BASE`, `_get_cache_path()`, `_extract_report_year()`)

**Details:**
- `_REPORTS_BASE = Path("output/reports")` defines the root cache directory.
- `_get_cache_path(stock_code, report_year, report_type)` returns a structured path: `output/reports/{stock_code}/{year}_{code}_{type}.md`.
- `_extract_report_year(title, report_date)` parses the 4-digit year from a Chinese report title (e.g. `2024å¹´`) and falls back to the year portion of `report_date` if no year is found in the title.
- Both functions inserted after `_SKIP_CHAPTER_KEYWORDS` block, before `_TOC_ENTRY_RE`.

## 2026-02-20 â€” Add report cache DB lookup and write helpers

**What:** Added `_check_report_cache()` and `_save_report_cache()` async functions to `tools/sina_reports.py` for DB-backed caching of distilled annual/quarterly reports.

**Files:**
- `tools/sina_reports.py` â€” modified (two async functions inserted after `_extract_report_year`)

**Details:**
- `_check_report_cache` queries the `report_cache` table by `(stock_code, report_type, report_year)` and validates the cached filepath exists on disk before returning it; returns `None` on any miss or error.
- `_save_report_cache` upserts a row into `report_cache`, updating `filepath`, `report_date`, `title`, and `source_url` on conflict; silently swallows exceptions so cache failures never break the main flow.
- Both functions import `db.get_pool` lazily inside the function body to avoid circular imports at module load time.

## 2026-02-20 â€” Wire report cache into fetch_company_report

**What:** Replaced `fetch_company_report()` with a new version that adds a fast cache path (DB check before any network fetch) and saves distilled reports to disk + DB on cache miss.

**Files:**
- `tools/sina_reports.py` â€” modified (fetch_company_report replaced entirely)

**Details:**
- Fast path: calls `_extract_report_year` + `_check_report_cache`; on hit, reads the local `.md` file and returns immediately with `summarized_by: "cache"`
- Slow path (cache miss): fetches detail page, extracts text/tables, calls `_grok_summarize_report`, builds a Markdown document with a metadata header, writes it via `_get_cache_path` + `_save_report_cache`
- Return dict now includes `cache_path` field on both paths
- `pdf_url` field retained in slow-path return; omitted on cache-hit path (not re-parsed)

## 2026-02-20 â€” Annual report distillation and cache system

**What:** Added TOC-aware pre-filtering, structured Grok prompt (Option B), and a local Markdown cache for distilled annual reports â€” eliminating redundant Sina/Grok fetches and fixing the 80k-char truncation that was cutting off ç¬¬åèŠ‚è´¢åŠ¡æŠ¥å‘Š.

**Files:**
- `db.py` â€” modified: added `report_cache` table and index to SCHEMA_SQL
- `tools/sina_reports.py` â€” modified: added `_KEEP/SKIP_CHAPTER_KEYWORDS`, `_TOC_ENTRY_RE`, `_CHAPTER_HEADING_RE`, `_should_keep_chapter`, `_parse_toc`, `_filter_sections_by_toc`, `_get_cache_path`, `_extract_report_year`, `_check_report_cache`, `_save_report_cache`; replaced `_prepare_for_grok`, `_grok_summarize_report`, `fetch_company_report`
- `output/reports/` â€” created at runtime per-request via `Path.mkdir(parents=True, exist_ok=True)`
- `docs/plans/2026-02-20-report-cache.md` â€” created: implementation plan

**Details:**
- TOC filter parses the ç›®å½• block (first 400 lines) and removes skip-chapters (å…¬å¸æ²»ç†, ç¯å¢ƒç¤¾ä¼šè´£ä»», etc.) by chapter name keywords before deduplication â€” typically 40â€“60% reduction for yearly reports
- Keep-keywords checked before skip-keywords so "å…¬å¸ç®€ä»‹å’Œä¸»è¦è´¢åŠ¡æŒ‡æ ‡" is correctly kept
- Grok prompt now requests 5-section structured Markdown with full financial tables preserved (Option B)
- `fetch_company_report` gains a fast path: cache hit â†’ read local .md file, no HTTP/Grok calls
- Cache stored at `output/reports/{stock_code}/{year}_{code}_{type}.md`; DB entry in `report_cache` table
- All cache operations are best-effort (silently fail, never crash the fetch)
- ç¬¬åèŠ‚è´¢åŠ¡æŠ¥å‘Š (previously truncated at 80k chars) now reaches Grok intact

## 2026-02-21 â€” Voice input (STT) on main chat page

**What:** Added a mic button to the chat input area that records audio, sends it to Whisper, and fills the textarea with the transcription.

**Files:**
- `api_chat.py` â€” added `POST /api/chat/stt` endpoint (authenticated, calls Whisper whisper-1, zh language)
- `frontend/src/components/ChatView.tsx` â€” added `voiceState`, `handleVoiceToggle`, and mic button in `input-area`
- `frontend/src/styles/index.css` â€” added `.mic-btn` and `.mic-btn.recording` styles with pulse animation
- `frontend/dist/` â€” rebuilt

**Details:**
- Mic button sits between textarea and send button; shows ğŸ¤ / â¹ / â€¦ states
- Recording stops on second click; transcription fills textarea for user review before sending
- Button disabled during transcription or while agent is running

## 2026-02-21 â€” STT stock resolution pipeline shared + wired into production

**What:** Extracted GPT extraction + fuzzy pinyin matching into a shared module; wired it into the production STT endpoint so the main app resolves stock names on voice input.

**Files:**
- `tools/stt_stocks.py` â€” created; shared module with `to_pinyin`, `levenshtein`, `extract_and_find_stocks`
- `api_chat.py` â€” STT endpoint now calls full pipeline, returns `matched_stocks` alongside `text`
- `tests/test_whisper_web.py` â€” refactored to import from `tools/stt_stocks` instead of duplicating logic
- `frontend/src/components/ChatView.tsx` â€” voice handler appends confident matches (distance â‰¤ 1) to textarea text
- `frontend/dist/` â€” rebuilt

**Details:**
- Confident matches (edit distance â‰¤ 1) are appended as e.g. `é£è¯­ç­‘(300873.SZ)` so agent gets exact stock code
- Higher-distance matches are returned from API but not auto-appended (user can still edit textarea)

## 2026-02-21 â€” Unified theme + comfort redesign

**What:** Aligned the landing page and main app to share the same visual identity. Refined the main app with a "comfort touch" â€” warmer message bubbles, better typography, sidebar brand header, and softer styling throughout.

**Files:**
- `frontend/src/styles/index.css` â€” full redesign: Google Fonts import, DM Sans UI font, softer message bubble shadows, refined spacing, sidebar brand, accent-gold status dots, warmer empty state
- `frontend/src/styles/landing.css` â€” warmed all dark backgrounds from cold #09080a â†’ #0c0907 family (warm dark brown), features section and footer updated to match
- `frontend/src/components/Sidebar.tsx` â€” added `.sidebar-brand` block with "é‡‘èç ”ç©¶æ™ºèƒ½ä½“" in Noto Serif SC gold (matching landing nav logo); converted sidebar-links from inline styles to CSS class

**Details:**
- Both pages now share Playfair Display + Noto Serif SC + DM Sans font trio (same @import in both CSS files)
- Sidebar brand "é‡‘èç ”ç©¶æ™ºèƒ½ä½“" in Noto Serif SC creates direct visual continuity with landing page logo
- Assistant messages: removed harsh border, replaced with subtle box-shadow for paper-like warmth
- User messages: more padding, softer rounded corners (14px)
- Input textarea: Georgia/Noto Serif SC for writing feel; focus ring instead of just border
- Status dots now use --accent-gold (#c9a227) matching landing page accent
- Debate modal: warmer backdrop, softer border-radius
- Added `--accent-gold` and `--shadow-soft` CSS variables for consistency

## 2026-02-21 â€” Dark/light theme toggle

**What:** Added a persistent dark/light theme toggle to the main app and landing page.

**Files:**
- `frontend/src/i18n.tsx` â€” added `ThemeProvider` + `useTheme` hook; applies `data-theme` attribute to `<html>` and persists to localStorage
- `frontend/src/main.tsx` â€” wrapped app in `ThemeProvider`
- `frontend/src/styles/index.css` â€” added `:root[data-theme="dark"]` CSS variable overrides + specific dark mode rules for hardcoded `#fff` inputs
- `frontend/src/components/Sidebar.tsx` â€” added ğŸŒ™/â˜€ toggle button in the sidebar footer
- `frontend/src/pages/LandingPage.tsx` â€” added ğŸŒ™/â˜€ toggle button in the landing nav (sets preference before login)

**Details:**
- Dark theme reuses the same warm dark palette as the landing page (#0c0907 background, #c9a227 gold accent)
- Preference persists in localStorage as "theme"
- `data-theme="dark"` on `<html>` allows CSS variable overrides without JS class manipulation
- Hardcoded `#fff` input backgrounds are overridden for dark mode via `[data-theme="dark"]` selectors

## 2026-02-22 â€” CSS: mode badge, header indicator, and mode picker styles

**What:** Added CSS styles for three new UI elements supporting per-conversation mode selection.

**Files:**
- `frontend/src/styles/index.css` â€” modified (added `position: relative` to `.chat-view`; appended styles for `.conv-mode-badge`, `.mode-indicator`, `.mode-indicator.debate`, `.mode-picker-overlay`, `.mode-picker`, `.mode-picker-prompt`, `.mode-picker-buttons`, `.mode-picker-btn`, `.mode-picker-btn.debate`, `.mode-picker-btn.normal`, `.mode-picker-cancel`)

**Details:**
- `.chat-view` now has `position: relative` so the `.mode-picker-overlay` (`position: absolute; inset: 0`) has a positioned ancestor
- `.conv-mode-badge` is a small inline tag in the sidebar conversation list for debate-mode conversations
- `.mode-indicator` is a thin strip below the chat header showing the current conversation mode; `.mode-indicator.debate` applies a warm gold tint
- `.mode-picker-*` is an inline overlay prompt that appears in the chat area on first send, asking the user to choose debate vs normal mode

## 2026-02-22 â€” Conversation mode indicator

**What:** Added per-conversation debate/normal mode badge in sidebar and header strip in chat area; follow-up messages in debate conversations prompt for mode choice.

**Files:**
- `db.py` â€” added `mode TEXT DEFAULT 'normal'` column migration
- `accounts.py` â€” `_create_conversation` and `new_conversation` accept `mode` param
- `api_chat.py` â€” `list_conversations` returns `mode`; `create_conversation` accepts mode body
- `frontend/src/api.ts` â€” `createConversation` sends mode in request body
- `frontend/src/pages/ChatLayout.tsx` â€” derives `activeMode`; passes to Sidebar + ChatView; renders mode strip
- `frontend/src/components/Sidebar.tsx` â€” renders `[è¾©è®º]`/`[Debate]` badge per conversation
- `frontend/src/components/ChatView.tsx` â€” intercepts send in debate convos with messages; shows mode picker
- `frontend/src/styles/index.css` â€” styles for badge, mode strip, and mode picker overlay

**Details:**
- Mode stored in DB; all existing conversations default to 'normal'
- Debate conversations created with mode='debate' upfront (not inferred from title)
- Mode picker only triggers in debate conversations with existing messages; first message in any conversation sends directly
- Cancel in mode picker restores the typed message to input

## 2026-02-23 â€” Async OHLCV ingest with rich progress

**What:** Rewrote ingest_ohlcv.py to use asyncio + ProcessPoolExecutor for parallel BaoStock fetching and asyncpg for async DB writes, with a rich progress bar.

**Files:**
- `data/ingest_ohlcv.py` â€” rewritten: asyncio main loop, ProcessPoolExecutor workers (each with own BaoStock login), asyncpg pool replaces psycopg2, rich Progress bar with ETA/count/rows
- `requirements.txt` â€” added `rich`

**Details:**
- CONCURRENCY env var (default 3) controls parallel BaoStock workers; each subprocess has its own bs.login() session
- Main process only logs into BaoStock to fetch the stock list, then logs out; workers handle all data fetching
- Removed time.sleep(0.1) per stock; rate limiting now implicit via semaphore + process count
- Progress bar shows: N/M complete, bar, %, elapsed, ETA, current stock + row count
- ON CONFLICT DO NOTHING preserved for safe re-runs after interruption

## 2026-02-23 â€” OpenBB global market data tool

**What:** Added fetch_global_market_data tool wrapping the full OpenBB Platform API for on-demand access to global financial data.

**Files:**
- `tools/openbb_data.py` â€” created: tool implementation + comprehensive schema covering all OpenBB namespaces
- `tools/__init__.py` â€” registered new tool in TOOL_SCHEMAS and TOOL_MAP
- `requirements.txt` â€” added openbb

**Details:**
- Single dynamic tool: command string (e.g. "equity.price.historical") + params dict
- OpenBB initialized once at import via lru_cache; FRED_API_KEY and FMP_API_KEY injected via direct attribute assignment on obb.user.credentials
- Covers: equity (prices, fundamentals, filings), economy (GDP, CPI, calendar), fixed income (yield curve, treasury rates), forex, crypto, ETF, index, options, news, SEC/CFTC regulators
- Primary providers: fmp (equity, ETF, index, forex, crypto), fred/federal_reserve (rates, fixed income), oecd (macro/GDP), sec (filings)
- yfinance excluded (blocked on server); all providers confirmed working
- 60s timeout, 5min cache, 150-row / 12k-char output cap

## 2026-02-23 â€” Update planning and system prompts for TA strategy tools

**What:** Added TA strategy tool entries to the planning prompt tool table and replaced the old K-line guidance bullet with a step-by-step TA workflow; added citation mappings for the new tools in the system prompt.

**Files:**
- `config.py` â€” modified (planning prompt table, TA guidance section, system prompt citation mapping)

**Details:**
- Tool table: added rows for `lookup_ta_strategy`, `save_ta_strategy`, `update_ta_strategy`, `run_ta_script` after `fetch_ohlcv`.
- TA guidance: replaced single bullet with a 4-step workflow covering knowledge-base lookup, optional web_search + save, OHLCV fetch, and pandas-ta script execution via `run_ta_script`.
- Citation mapping: added `run_ta_script â†’ https://pypi.org/project/pandas-ta/` and a no-citation note for the three knowledge-base tools.

## 2026-02-23 â€” TA Strategy System

**What:** Added TA strategy knowledge base and sandboxed code-execution tool for pandas-ta + Plotly chart generation.

**Files:**
- `db.py` â€” modified: added `ta_strategies` table + FTS index to SCHEMA_SQL
- `tools/ta_strategies.py` â€” created: lookup_ta_strategy, save_ta_strategy, update_ta_strategy tools
- `tools/ta_executor.py` â€” created: run_ta_script tool (subprocess sandbox, 3-attempt retry via MiniMax)
- `tools/__init__.py` â€” modified: registered 4 new tools in TOOL_SCHEMAS and TOOL_MAP
- `config.py` â€” modified: updated planning prompt with TA workflow guidance; updated system prompt citation mapping
- `frontend/src/components/MessageBubble.tsx` â€” modified: HTML chart opens in new tab
- `frontend/src/components/ReportsPanel.tsx` â€” modified: html filter tab + open-in-tab handler
- `structure.md` â€” modified: documented new tools and ta_strategies DB table
- `requirements.txt` â€” modified: added pandas-ta, plotly
- `tests/test_ta_strategies.py` â€” created: 5 unit tests (mocked DB)
- `tests/test_ta_executor.py` â€” created: 5 unit tests (mocked subprocess + MiniMax)

**Details:**
- Strategies stored in myaiagent Postgres (not marketdata DB)
- Import sandbox uses frame inspection (sys._getframe) to only restrict user script imports, not library internals
- Subprocess timeout: 30 seconds; asyncio.to_thread prevents blocking uvicorn event loop
- On failure: MiniMax rewrites script, up to 3 total attempts
- run_ta_script returns {"file": path} â€” picked up by agent.py line 492 automatically
- Strategy alterations via update_ta_strategy (patches fields, never deletes)
- Plotly .html charts open in new browser tab from both chat and ReportsPanel

## 2026-02-23 â€” Add HTML Plotly chart support to frontend (Task 3b)

**What:** Updated MessageBubble and ReportsPanel to render interactive Plotly HTML charts, and added the corresponding i18n key.

**Files:**
- `frontend/src/components/MessageBubble.tsx` â€” modified: added `.html` case that renders a link opening the chart in a new tab
- `frontend/src/components/ReportsPanel.tsx` â€” modified: renamed `handleDownload` to `handleOpen`, added html case that opens chart in new tab, added "html" filter tab, updated icon to show ğŸ“Š for html files
- `frontend/src/i18n.tsx` â€” modified: added `"reports.charts_interactive"` translation key (en: "Charts", zh: "äº’åŠ¨å›¾è¡¨")

**Details:**
- HTML charts open via `window.open` with the token as a query param (no Authorization header needed since the file is served directly).
- The new filter tab appears after "Markdown" in ReportsPanel.
- TypeScript check (`tsc --noEmit`) passed with zero errors.
